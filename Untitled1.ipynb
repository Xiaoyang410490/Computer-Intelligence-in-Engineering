{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "730c23fb",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "import torch\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "import torch.nn as nn\n",
    "from torch.utils.data import Dataset,DataLoader"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f62d39a1",
   "metadata": {},
   "source": [
    "Read the dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "d0af229a",
   "metadata": {},
   "outputs": [],
   "source": [
    "f = open(\"num_input_(976, 100, 4)_0.2.pickle\",'rb')\n",
    "x_num = pickle.load(f)\n",
    "f.close()\n",
    "\n",
    "f1 = open(\"num_target_(976, 2)_0.2.pickle\",'rb')\n",
    "y_num = pickle.load(f1)\n",
    "f1.close()\n",
    "\n",
    "f2 = open(\"exp_input_(24, 100, 4)_0.2.pickle\", 'rb')\n",
    "x_exp = pickle.load(f2)\n",
    "f2.close()\n",
    "\n",
    "f3 = open(\"exp_target_(24, 2)_0.2.pickle\", 'rb')\n",
    "y_exp = pickle.load(f3)\n",
    "f3.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "0769ac55",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(780, 100, 4)\n",
      "(98, 100, 4)\n",
      "(98, 100, 4)\n",
      "(780, 2)\n",
      "(98, 2)\n",
      "(98, 2)\n"
     ]
    }
   ],
   "source": [
    "x_train = x_num[:int(0.8*x_num.shape[0]),:,:]\n",
    "x_validation = x_num[int(0.8*x_num.shape[0]):int(0.9*x_num.shape[0]),:,:]\n",
    "x_test = x_num[int(0.9*x_num.shape[0]):,:,:]\n",
    "\n",
    "y_train = y_num[:int(0.8*y_num.shape[0]),:]\n",
    "y_validation = y_num[int(0.8*y_num.shape[0]):int(0.9*y_num.shape[0]),:]\n",
    "y_test = y_num[int(0.9*y_num.shape[0]):,:]\n",
    "\n",
    "print(x_train.shape)\n",
    "print(x_validation.shape)\n",
    "print(x_test.shape)\n",
    "print(y_train.shape)\n",
    "print(y_validation.shape)\n",
    "print(y_test.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "08ad1b5a",
   "metadata": {},
   "source": [
    "Expand the dimension for the convolutional neural network (CNN). For Pytorch, we need to expand the first dimension (Compared to Tensorflow, where we need to expand the third dimension) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "38aba16d",
   "metadata": {},
   "outputs": [],
   "source": [
    "x_trainc = np.expand_dims(x_train,1)\n",
    "x_validationc = np.expand_dims(x_validation,1)\n",
    "x_testc = np.expand_dims(x_test,1)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b58db46b",
   "metadata": {},
   "source": [
    "Dataset Normalization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "76566f3b",
   "metadata": {},
   "outputs": [],
   "source": [
    "scaler = MinMaxScaler()\n",
    "y_trainr = scaler.fit_transform(y_train)\n",
    "y_validationr = scaler.transform(y_validation)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "91e8a99f",
   "metadata": {},
   "source": [
    "Get the dataset for Pytorch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "36e6f019",
   "metadata": {},
   "outputs": [],
   "source": [
    "class SignalDataset(Dataset):\n",
    "    def __init__(self, inputx, outputy):\n",
    "        self.input_x = inputx\n",
    "        self.output_y = outputy\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.input_x)\n",
    "\n",
    "    def __getitem__(self,index):\n",
    "\n",
    "        return self.input_x[index,:], self.output_y[index]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "4806f8cc",
   "metadata": {},
   "outputs": [],
   "source": [
    "training_set = SignalDataset(x_trainc,y_trainr)\n",
    "validation_set = SignalDataset(x_validationc,y_validationr)\n",
    "test_set = SignalDataset(x_testc,y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "a87e87d0",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_dataloader = DataLoader(training_set, batch_size=32, shuffle=True)\n",
    "validation_dataloader = DataLoader(validation_set, batch_size=32, shuffle=False)\n",
    "test_dataloader = DataLoader(test_set, batch_size=1, shuffle=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ea1fdcba",
   "metadata": {},
   "source": [
    "Build the convolutional neural network (CNN)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "de1239b2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using cuda device\n",
      "NeuralNetwork(\n",
      "  (conv1): Conv2d(1, 3, kernel_size=(50, 2), stride=(1, 1))\n",
      "  (pool1): MaxPool2d(kernel_size=(3, 1), stride=(3, 1), padding=0, dilation=1, ceil_mode=False)\n",
      "  (conv2): Conv2d(3, 6, kernel_size=(10, 2), stride=(1, 1))\n",
      "  (pool2): MaxPool2d(kernel_size=(2, 1), stride=(2, 1), padding=0, dilation=1, ceil_mode=False)\n",
      "  (relu): ReLU()\n",
      "  (fc1): Linear(in_features=48, out_features=16, bias=True)\n",
      "  (fc2): Linear(in_features=16, out_features=2, bias=True)\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "# Get cpu or gpu device for training.\n",
    "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "print(f\"Using {device} device\")\n",
    "\n",
    "# Define model\n",
    "class NeuralNetwork(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(NeuralNetwork, self).__init__()\n",
    "        self.conv1 = nn.Conv2d(1, 3, (50,2))\n",
    "        self.pool1 = nn.MaxPool2d((3,1), stride=(3,1))\n",
    "        self.conv2 = nn.Conv2d(3, 6, (10,2))\n",
    "        self.pool2 = nn.MaxPool2d((2,1),stride=(2,1))\n",
    "        self.relu = nn.ReLU()\n",
    "        self.fc1 = nn.Linear(48, 16)\n",
    "        self.fc2 = nn.Linear(16, 2)\n",
    "    \n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.conv1(x)\n",
    "        x = self.pool1(x)\n",
    "        x = self.relu(x)\n",
    "        x = self.conv2(x)\n",
    "        x = self.pool2(x)\n",
    "        x = self.relu(x)\n",
    "        x = torch.flatten(x, 1)\n",
    "        \n",
    "        x = self.relu(self.fc1(x))\n",
    "        predict = self.relu(self.fc2(x))\n",
    "        \n",
    "        return predict\n",
    "\n",
    "model = NeuralNetwork().to(device)\n",
    "print(model)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cabd6198",
   "metadata": {},
   "source": [
    "The training process"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "9b82d087",
   "metadata": {},
   "outputs": [],
   "source": [
    "loss_fn = torch.nn.MSELoss()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "27fd6e04",
   "metadata": {},
   "outputs": [],
   "source": [
    "optimizer = torch.optim.Adam(model.parameters(),lr=0.001)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "52da9a11",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(dataloader, model, loss_fn, optimizer):\n",
    "    size = len(dataloader.dataset)\n",
    "    model.train()\n",
    "    for batch, (X, y) in enumerate(dataloader):\n",
    "        X = X.float()\n",
    "        y = y.float()\n",
    "        X, y = X.to(device), y.to(device)\n",
    "\n",
    "        # Compute prediction error\n",
    "        pred = model(X)\n",
    "        loss = loss_fn(pred, y)\n",
    "\n",
    "        # Backpropagation\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        if batch % 100 == 0:\n",
    "            loss, current = loss.item(), batch * len(X)\n",
    "            print(f\"loss: {loss:>7f}  [{current:>5d}/{size:>5d}]\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "1ba8d19a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def validate(dataloader, model, loss_fn):\n",
    "    size = len(dataloader.dataset)\n",
    "    num_batches = len(dataloader)\n",
    "    model.eval()\n",
    "    validation_loss = 0\n",
    "    with torch.no_grad():\n",
    "        for X, y in dataloader:\n",
    "            X = X.float()\n",
    "            y = y.float()\n",
    "            X, y = X.to(device), y.to(device)\n",
    "            pred = model(X)\n",
    "            validation_loss += loss_fn(pred, y).item()\n",
    "    validation_loss /= num_batches\n",
    "    print(f\"Test Error: \\n  Avg loss: {validation_loss:>8f} \\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "5d90e8cd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1\n",
      "-------------------------------\n",
      "loss: 0.269407  [    0/  780]\n",
      "Test Error: \n",
      "  Avg loss: 0.080474 \n",
      "\n",
      "Epoch 2\n",
      "-------------------------------\n",
      "loss: 0.079604  [    0/  780]\n",
      "Test Error: \n",
      "  Avg loss: 0.061963 \n",
      "\n",
      "Epoch 3\n",
      "-------------------------------\n",
      "loss: 0.058492  [    0/  780]\n",
      "Test Error: \n",
      "  Avg loss: 0.032173 \n",
      "\n",
      "Epoch 4\n",
      "-------------------------------\n",
      "loss: 0.064445  [    0/  780]\n",
      "Test Error: \n",
      "  Avg loss: 0.018438 \n",
      "\n",
      "Epoch 5\n",
      "-------------------------------\n",
      "loss: 0.016748  [    0/  780]\n",
      "Test Error: \n",
      "  Avg loss: 0.013573 \n",
      "\n",
      "Epoch 6\n",
      "-------------------------------\n",
      "loss: 0.016290  [    0/  780]\n",
      "Test Error: \n",
      "  Avg loss: 0.013241 \n",
      "\n",
      "Epoch 7\n",
      "-------------------------------\n",
      "loss: 0.009534  [    0/  780]\n",
      "Test Error: \n",
      "  Avg loss: 0.010667 \n",
      "\n",
      "Epoch 8\n",
      "-------------------------------\n",
      "loss: 0.011798  [    0/  780]\n",
      "Test Error: \n",
      "  Avg loss: 0.009446 \n",
      "\n",
      "Epoch 9\n",
      "-------------------------------\n",
      "loss: 0.013620  [    0/  780]\n",
      "Test Error: \n",
      "  Avg loss: 0.007741 \n",
      "\n",
      "Epoch 10\n",
      "-------------------------------\n",
      "loss: 0.010150  [    0/  780]\n",
      "Test Error: \n",
      "  Avg loss: 0.007471 \n",
      "\n",
      "Epoch 11\n",
      "-------------------------------\n",
      "loss: 0.007632  [    0/  780]\n",
      "Test Error: \n",
      "  Avg loss: 0.006809 \n",
      "\n",
      "Epoch 12\n",
      "-------------------------------\n",
      "loss: 0.007739  [    0/  780]\n",
      "Test Error: \n",
      "  Avg loss: 0.007628 \n",
      "\n",
      "Epoch 13\n",
      "-------------------------------\n",
      "loss: 0.005096  [    0/  780]\n",
      "Test Error: \n",
      "  Avg loss: 0.006303 \n",
      "\n",
      "Epoch 14\n",
      "-------------------------------\n",
      "loss: 0.008182  [    0/  780]\n",
      "Test Error: \n",
      "  Avg loss: 0.006338 \n",
      "\n",
      "Epoch 15\n",
      "-------------------------------\n",
      "loss: 0.003383  [    0/  780]\n",
      "Test Error: \n",
      "  Avg loss: 0.005048 \n",
      "\n",
      "Epoch 16\n",
      "-------------------------------\n",
      "loss: 0.004927  [    0/  780]\n",
      "Test Error: \n",
      "  Avg loss: 0.005523 \n",
      "\n",
      "Epoch 17\n",
      "-------------------------------\n",
      "loss: 0.004593  [    0/  780]\n",
      "Test Error: \n",
      "  Avg loss: 0.005715 \n",
      "\n",
      "Epoch 18\n",
      "-------------------------------\n",
      "loss: 0.004481  [    0/  780]\n",
      "Test Error: \n",
      "  Avg loss: 0.004484 \n",
      "\n",
      "Epoch 19\n",
      "-------------------------------\n",
      "loss: 0.004162  [    0/  780]\n",
      "Test Error: \n",
      "  Avg loss: 0.004980 \n",
      "\n",
      "Epoch 20\n",
      "-------------------------------\n",
      "loss: 0.002529  [    0/  780]\n",
      "Test Error: \n",
      "  Avg loss: 0.004150 \n",
      "\n",
      "Epoch 21\n",
      "-------------------------------\n",
      "loss: 0.003450  [    0/  780]\n",
      "Test Error: \n",
      "  Avg loss: 0.003636 \n",
      "\n",
      "Epoch 22\n",
      "-------------------------------\n",
      "loss: 0.003086  [    0/  780]\n",
      "Test Error: \n",
      "  Avg loss: 0.004062 \n",
      "\n",
      "Epoch 23\n",
      "-------------------------------\n",
      "loss: 0.003219  [    0/  780]\n",
      "Test Error: \n",
      "  Avg loss: 0.003856 \n",
      "\n",
      "Epoch 24\n",
      "-------------------------------\n",
      "loss: 0.003930  [    0/  780]\n",
      "Test Error: \n",
      "  Avg loss: 0.003216 \n",
      "\n",
      "Epoch 25\n",
      "-------------------------------\n",
      "loss: 0.003445  [    0/  780]\n",
      "Test Error: \n",
      "  Avg loss: 0.003283 \n",
      "\n",
      "Epoch 26\n",
      "-------------------------------\n",
      "loss: 0.001968  [    0/  780]\n",
      "Test Error: \n",
      "  Avg loss: 0.002939 \n",
      "\n",
      "Epoch 27\n",
      "-------------------------------\n",
      "loss: 0.002792  [    0/  780]\n",
      "Test Error: \n",
      "  Avg loss: 0.002759 \n",
      "\n",
      "Epoch 28\n",
      "-------------------------------\n",
      "loss: 0.001541  [    0/  780]\n",
      "Test Error: \n",
      "  Avg loss: 0.002666 \n",
      "\n",
      "Epoch 29\n",
      "-------------------------------\n",
      "loss: 0.001743  [    0/  780]\n",
      "Test Error: \n",
      "  Avg loss: 0.002522 \n",
      "\n",
      "Epoch 30\n",
      "-------------------------------\n",
      "loss: 0.001655  [    0/  780]\n",
      "Test Error: \n",
      "  Avg loss: 0.002750 \n",
      "\n",
      "Epoch 31\n",
      "-------------------------------\n",
      "loss: 0.002320  [    0/  780]\n",
      "Test Error: \n",
      "  Avg loss: 0.002189 \n",
      "\n",
      "Epoch 32\n",
      "-------------------------------\n",
      "loss: 0.002019  [    0/  780]\n",
      "Test Error: \n",
      "  Avg loss: 0.002255 \n",
      "\n",
      "Epoch 33\n",
      "-------------------------------\n",
      "loss: 0.001703  [    0/  780]\n",
      "Test Error: \n",
      "  Avg loss: 0.002321 \n",
      "\n",
      "Epoch 34\n",
      "-------------------------------\n",
      "loss: 0.001675  [    0/  780]\n",
      "Test Error: \n",
      "  Avg loss: 0.001748 \n",
      "\n",
      "Epoch 35\n",
      "-------------------------------\n",
      "loss: 0.002090  [    0/  780]\n",
      "Test Error: \n",
      "  Avg loss: 0.001837 \n",
      "\n",
      "Epoch 36\n",
      "-------------------------------\n",
      "loss: 0.002296  [    0/  780]\n",
      "Test Error: \n",
      "  Avg loss: 0.001724 \n",
      "\n",
      "Epoch 37\n",
      "-------------------------------\n",
      "loss: 0.001106  [    0/  780]\n",
      "Test Error: \n",
      "  Avg loss: 0.001776 \n",
      "\n",
      "Epoch 38\n",
      "-------------------------------\n",
      "loss: 0.001218  [    0/  780]\n",
      "Test Error: \n",
      "  Avg loss: 0.001749 \n",
      "\n",
      "Epoch 39\n",
      "-------------------------------\n",
      "loss: 0.001645  [    0/  780]\n",
      "Test Error: \n",
      "  Avg loss: 0.001851 \n",
      "\n",
      "Epoch 40\n",
      "-------------------------------\n",
      "loss: 0.001614  [    0/  780]\n",
      "Test Error: \n",
      "  Avg loss: 0.001815 \n",
      "\n",
      "Epoch 41\n",
      "-------------------------------\n",
      "loss: 0.001442  [    0/  780]\n",
      "Test Error: \n",
      "  Avg loss: 0.001579 \n",
      "\n",
      "Epoch 42\n",
      "-------------------------------\n",
      "loss: 0.001482  [    0/  780]\n",
      "Test Error: \n",
      "  Avg loss: 0.001729 \n",
      "\n",
      "Epoch 43\n",
      "-------------------------------\n",
      "loss: 0.001383  [    0/  780]\n",
      "Test Error: \n",
      "  Avg loss: 0.001607 \n",
      "\n",
      "Epoch 44\n",
      "-------------------------------\n",
      "loss: 0.001207  [    0/  780]\n",
      "Test Error: \n",
      "  Avg loss: 0.001403 \n",
      "\n",
      "Epoch 45\n",
      "-------------------------------\n",
      "loss: 0.001080  [    0/  780]\n",
      "Test Error: \n",
      "  Avg loss: 0.001798 \n",
      "\n",
      "Epoch 46\n",
      "-------------------------------\n",
      "loss: 0.001354  [    0/  780]\n",
      "Test Error: \n",
      "  Avg loss: 0.001629 \n",
      "\n",
      "Epoch 47\n",
      "-------------------------------\n",
      "loss: 0.001330  [    0/  780]\n",
      "Test Error: \n",
      "  Avg loss: 0.001459 \n",
      "\n",
      "Epoch 48\n",
      "-------------------------------\n",
      "loss: 0.001047  [    0/  780]\n",
      "Test Error: \n",
      "  Avg loss: 0.001342 \n",
      "\n",
      "Epoch 49\n",
      "-------------------------------\n",
      "loss: 0.001378  [    0/  780]\n",
      "Test Error: \n",
      "  Avg loss: 0.001333 \n",
      "\n",
      "Epoch 50\n",
      "-------------------------------\n",
      "loss: 0.001422  [    0/  780]\n",
      "Test Error: \n",
      "  Avg loss: 0.001192 \n",
      "\n",
      "Epoch 51\n",
      "-------------------------------\n",
      "loss: 0.001171  [    0/  780]\n",
      "Test Error: \n",
      "  Avg loss: 0.001239 \n",
      "\n",
      "Epoch 52\n",
      "-------------------------------\n",
      "loss: 0.001058  [    0/  780]\n",
      "Test Error: \n",
      "  Avg loss: 0.001011 \n",
      "\n",
      "Epoch 53\n",
      "-------------------------------\n",
      "loss: 0.001493  [    0/  780]\n",
      "Test Error: \n",
      "  Avg loss: 0.001098 \n",
      "\n",
      "Epoch 54\n",
      "-------------------------------\n",
      "loss: 0.000771  [    0/  780]\n",
      "Test Error: \n",
      "  Avg loss: 0.001174 \n",
      "\n",
      "Epoch 55\n",
      "-------------------------------\n",
      "loss: 0.000897  [    0/  780]\n",
      "Test Error: \n",
      "  Avg loss: 0.001522 \n",
      "\n",
      "Epoch 56\n",
      "-------------------------------\n",
      "loss: 0.000998  [    0/  780]\n",
      "Test Error: \n",
      "  Avg loss: 0.001214 \n",
      "\n",
      "Epoch 57\n",
      "-------------------------------\n",
      "loss: 0.000974  [    0/  780]\n",
      "Test Error: \n",
      "  Avg loss: 0.000968 \n",
      "\n",
      "Epoch 58\n",
      "-------------------------------\n",
      "loss: 0.001330  [    0/  780]\n",
      "Test Error: \n",
      "  Avg loss: 0.001143 \n",
      "\n",
      "Epoch 59\n",
      "-------------------------------\n",
      "loss: 0.000547  [    0/  780]\n",
      "Test Error: \n",
      "  Avg loss: 0.001301 \n",
      "\n",
      "Epoch 60\n",
      "-------------------------------\n",
      "loss: 0.001210  [    0/  780]\n",
      "Test Error: \n",
      "  Avg loss: 0.001180 \n",
      "\n",
      "Epoch 61\n",
      "-------------------------------\n",
      "loss: 0.001042  [    0/  780]\n",
      "Test Error: \n",
      "  Avg loss: 0.001293 \n",
      "\n",
      "Epoch 62\n",
      "-------------------------------\n",
      "loss: 0.000790  [    0/  780]\n",
      "Test Error: \n",
      "  Avg loss: 0.000946 \n",
      "\n",
      "Epoch 63\n",
      "-------------------------------\n",
      "loss: 0.000969  [    0/  780]\n",
      "Test Error: \n",
      "  Avg loss: 0.000912 \n",
      "\n",
      "Epoch 64\n",
      "-------------------------------\n",
      "loss: 0.000957  [    0/  780]\n",
      "Test Error: \n",
      "  Avg loss: 0.001012 \n",
      "\n",
      "Epoch 65\n",
      "-------------------------------\n",
      "loss: 0.000770  [    0/  780]\n",
      "Test Error: \n",
      "  Avg loss: 0.001062 \n",
      "\n",
      "Epoch 66\n",
      "-------------------------------\n",
      "loss: 0.001233  [    0/  780]\n",
      "Test Error: \n",
      "  Avg loss: 0.001134 \n",
      "\n",
      "Epoch 67\n",
      "-------------------------------\n",
      "loss: 0.000884  [    0/  780]\n",
      "Test Error: \n",
      "  Avg loss: 0.001032 \n",
      "\n",
      "Epoch 68\n",
      "-------------------------------\n",
      "loss: 0.000870  [    0/  780]\n",
      "Test Error: \n",
      "  Avg loss: 0.001116 \n",
      "\n",
      "Epoch 69\n",
      "-------------------------------\n",
      "loss: 0.000934  [    0/  780]\n",
      "Test Error: \n",
      "  Avg loss: 0.000867 \n",
      "\n",
      "Epoch 70\n",
      "-------------------------------\n",
      "loss: 0.000568  [    0/  780]\n",
      "Test Error: \n",
      "  Avg loss: 0.000978 \n",
      "\n",
      "Epoch 71\n",
      "-------------------------------\n",
      "loss: 0.000520  [    0/  780]\n",
      "Test Error: \n",
      "  Avg loss: 0.000861 \n",
      "\n",
      "Epoch 72\n",
      "-------------------------------\n",
      "loss: 0.000674  [    0/  780]\n",
      "Test Error: \n",
      "  Avg loss: 0.000943 \n",
      "\n",
      "Epoch 73\n",
      "-------------------------------\n",
      "loss: 0.000853  [    0/  780]\n",
      "Test Error: \n",
      "  Avg loss: 0.001038 \n",
      "\n",
      "Epoch 74\n",
      "-------------------------------\n",
      "loss: 0.000793  [    0/  780]\n",
      "Test Error: \n",
      "  Avg loss: 0.001175 \n",
      "\n",
      "Epoch 75\n",
      "-------------------------------\n",
      "loss: 0.000673  [    0/  780]\n",
      "Test Error: \n",
      "  Avg loss: 0.000901 \n",
      "\n",
      "Epoch 76\n",
      "-------------------------------\n",
      "loss: 0.000978  [    0/  780]\n",
      "Test Error: \n",
      "  Avg loss: 0.000987 \n",
      "\n",
      "Epoch 77\n",
      "-------------------------------\n",
      "loss: 0.000919  [    0/  780]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test Error: \n",
      "  Avg loss: 0.000836 \n",
      "\n",
      "Epoch 78\n",
      "-------------------------------\n",
      "loss: 0.000689  [    0/  780]\n",
      "Test Error: \n",
      "  Avg loss: 0.000948 \n",
      "\n",
      "Epoch 79\n",
      "-------------------------------\n",
      "loss: 0.000698  [    0/  780]\n",
      "Test Error: \n",
      "  Avg loss: 0.001019 \n",
      "\n",
      "Epoch 80\n",
      "-------------------------------\n",
      "loss: 0.000629  [    0/  780]\n",
      "Test Error: \n",
      "  Avg loss: 0.001225 \n",
      "\n",
      "Epoch 81\n",
      "-------------------------------\n",
      "loss: 0.000561  [    0/  780]\n",
      "Test Error: \n",
      "  Avg loss: 0.001102 \n",
      "\n",
      "Epoch 82\n",
      "-------------------------------\n",
      "loss: 0.000656  [    0/  780]\n",
      "Test Error: \n",
      "  Avg loss: 0.000952 \n",
      "\n",
      "Epoch 83\n",
      "-------------------------------\n",
      "loss: 0.000411  [    0/  780]\n",
      "Test Error: \n",
      "  Avg loss: 0.000778 \n",
      "\n",
      "Epoch 84\n",
      "-------------------------------\n",
      "loss: 0.000719  [    0/  780]\n",
      "Test Error: \n",
      "  Avg loss: 0.000831 \n",
      "\n",
      "Epoch 85\n",
      "-------------------------------\n",
      "loss: 0.000502  [    0/  780]\n",
      "Test Error: \n",
      "  Avg loss: 0.000798 \n",
      "\n",
      "Epoch 86\n",
      "-------------------------------\n",
      "loss: 0.000728  [    0/  780]\n",
      "Test Error: \n",
      "  Avg loss: 0.000909 \n",
      "\n",
      "Epoch 87\n",
      "-------------------------------\n",
      "loss: 0.000489  [    0/  780]\n",
      "Test Error: \n",
      "  Avg loss: 0.000851 \n",
      "\n",
      "Epoch 88\n",
      "-------------------------------\n",
      "loss: 0.000814  [    0/  780]\n",
      "Test Error: \n",
      "  Avg loss: 0.000856 \n",
      "\n",
      "Epoch 89\n",
      "-------------------------------\n",
      "loss: 0.000776  [    0/  780]\n",
      "Test Error: \n",
      "  Avg loss: 0.001037 \n",
      "\n",
      "Epoch 90\n",
      "-------------------------------\n",
      "loss: 0.000534  [    0/  780]\n",
      "Test Error: \n",
      "  Avg loss: 0.000938 \n",
      "\n",
      "Epoch 91\n",
      "-------------------------------\n",
      "loss: 0.000533  [    0/  780]\n",
      "Test Error: \n",
      "  Avg loss: 0.000738 \n",
      "\n",
      "Epoch 92\n",
      "-------------------------------\n",
      "loss: 0.000427  [    0/  780]\n",
      "Test Error: \n",
      "  Avg loss: 0.000698 \n",
      "\n",
      "Epoch 93\n",
      "-------------------------------\n",
      "loss: 0.000752  [    0/  780]\n",
      "Test Error: \n",
      "  Avg loss: 0.000692 \n",
      "\n",
      "Epoch 94\n",
      "-------------------------------\n",
      "loss: 0.000564  [    0/  780]\n",
      "Test Error: \n",
      "  Avg loss: 0.000908 \n",
      "\n",
      "Epoch 95\n",
      "-------------------------------\n",
      "loss: 0.000420  [    0/  780]\n",
      "Test Error: \n",
      "  Avg loss: 0.000951 \n",
      "\n",
      "Epoch 96\n",
      "-------------------------------\n",
      "loss: 0.000799  [    0/  780]\n",
      "Test Error: \n",
      "  Avg loss: 0.000917 \n",
      "\n",
      "Epoch 97\n",
      "-------------------------------\n",
      "loss: 0.000479  [    0/  780]\n",
      "Test Error: \n",
      "  Avg loss: 0.000819 \n",
      "\n",
      "Epoch 98\n",
      "-------------------------------\n",
      "loss: 0.000526  [    0/  780]\n",
      "Test Error: \n",
      "  Avg loss: 0.000683 \n",
      "\n",
      "Epoch 99\n",
      "-------------------------------\n",
      "loss: 0.000397  [    0/  780]\n",
      "Test Error: \n",
      "  Avg loss: 0.000873 \n",
      "\n",
      "Epoch 100\n",
      "-------------------------------\n",
      "loss: 0.000652  [    0/  780]\n",
      "Test Error: \n",
      "  Avg loss: 0.000704 \n",
      "\n",
      "Epoch 101\n",
      "-------------------------------\n",
      "loss: 0.000432  [    0/  780]\n",
      "Test Error: \n",
      "  Avg loss: 0.000754 \n",
      "\n",
      "Epoch 102\n",
      "-------------------------------\n",
      "loss: 0.000476  [    0/  780]\n",
      "Test Error: \n",
      "  Avg loss: 0.001135 \n",
      "\n",
      "Epoch 103\n",
      "-------------------------------\n",
      "loss: 0.000449  [    0/  780]\n",
      "Test Error: \n",
      "  Avg loss: 0.000671 \n",
      "\n",
      "Epoch 104\n",
      "-------------------------------\n",
      "loss: 0.000545  [    0/  780]\n",
      "Test Error: \n",
      "  Avg loss: 0.000711 \n",
      "\n",
      "Epoch 105\n",
      "-------------------------------\n",
      "loss: 0.000470  [    0/  780]\n",
      "Test Error: \n",
      "  Avg loss: 0.000749 \n",
      "\n",
      "Epoch 106\n",
      "-------------------------------\n",
      "loss: 0.000758  [    0/  780]\n",
      "Test Error: \n",
      "  Avg loss: 0.000767 \n",
      "\n",
      "Epoch 107\n",
      "-------------------------------\n",
      "loss: 0.000434  [    0/  780]\n",
      "Test Error: \n",
      "  Avg loss: 0.000996 \n",
      "\n",
      "Epoch 108\n",
      "-------------------------------\n",
      "loss: 0.000651  [    0/  780]\n",
      "Test Error: \n",
      "  Avg loss: 0.000637 \n",
      "\n",
      "Epoch 109\n",
      "-------------------------------\n",
      "loss: 0.000590  [    0/  780]\n",
      "Test Error: \n",
      "  Avg loss: 0.000629 \n",
      "\n",
      "Epoch 110\n",
      "-------------------------------\n",
      "loss: 0.000458  [    0/  780]\n",
      "Test Error: \n",
      "  Avg loss: 0.000892 \n",
      "\n",
      "Epoch 111\n",
      "-------------------------------\n",
      "loss: 0.000517  [    0/  780]\n",
      "Test Error: \n",
      "  Avg loss: 0.000764 \n",
      "\n",
      "Epoch 112\n",
      "-------------------------------\n",
      "loss: 0.000626  [    0/  780]\n",
      "Test Error: \n",
      "  Avg loss: 0.000817 \n",
      "\n",
      "Epoch 113\n",
      "-------------------------------\n",
      "loss: 0.000540  [    0/  780]\n",
      "Test Error: \n",
      "  Avg loss: 0.000794 \n",
      "\n",
      "Epoch 114\n",
      "-------------------------------\n",
      "loss: 0.000601  [    0/  780]\n",
      "Test Error: \n",
      "  Avg loss: 0.000713 \n",
      "\n",
      "Epoch 115\n",
      "-------------------------------\n",
      "loss: 0.000441  [    0/  780]\n",
      "Test Error: \n",
      "  Avg loss: 0.000842 \n",
      "\n",
      "Epoch 116\n",
      "-------------------------------\n",
      "loss: 0.000565  [    0/  780]\n",
      "Test Error: \n",
      "  Avg loss: 0.000545 \n",
      "\n",
      "Epoch 117\n",
      "-------------------------------\n",
      "loss: 0.000431  [    0/  780]\n",
      "Test Error: \n",
      "  Avg loss: 0.000623 \n",
      "\n",
      "Epoch 118\n",
      "-------------------------------\n",
      "loss: 0.000779  [    0/  780]\n",
      "Test Error: \n",
      "  Avg loss: 0.000558 \n",
      "\n",
      "Epoch 119\n",
      "-------------------------------\n",
      "loss: 0.000396  [    0/  780]\n",
      "Test Error: \n",
      "  Avg loss: 0.000831 \n",
      "\n",
      "Epoch 120\n",
      "-------------------------------\n",
      "loss: 0.000389  [    0/  780]\n",
      "Test Error: \n",
      "  Avg loss: 0.000591 \n",
      "\n",
      "Epoch 121\n",
      "-------------------------------\n",
      "loss: 0.000517  [    0/  780]\n",
      "Test Error: \n",
      "  Avg loss: 0.000690 \n",
      "\n",
      "Epoch 122\n",
      "-------------------------------\n",
      "loss: 0.000585  [    0/  780]\n",
      "Test Error: \n",
      "  Avg loss: 0.000933 \n",
      "\n",
      "Epoch 123\n",
      "-------------------------------\n",
      "loss: 0.000871  [    0/  780]\n",
      "Test Error: \n",
      "  Avg loss: 0.000675 \n",
      "\n",
      "Epoch 124\n",
      "-------------------------------\n",
      "loss: 0.000442  [    0/  780]\n",
      "Test Error: \n",
      "  Avg loss: 0.000566 \n",
      "\n",
      "Epoch 125\n",
      "-------------------------------\n",
      "loss: 0.000386  [    0/  780]\n",
      "Test Error: \n",
      "  Avg loss: 0.000667 \n",
      "\n",
      "Epoch 126\n",
      "-------------------------------\n",
      "loss: 0.000449  [    0/  780]\n",
      "Test Error: \n",
      "  Avg loss: 0.000676 \n",
      "\n",
      "Epoch 127\n",
      "-------------------------------\n",
      "loss: 0.000556  [    0/  780]\n",
      "Test Error: \n",
      "  Avg loss: 0.000589 \n",
      "\n",
      "Epoch 128\n",
      "-------------------------------\n",
      "loss: 0.000331  [    0/  780]\n",
      "Test Error: \n",
      "  Avg loss: 0.000600 \n",
      "\n",
      "Epoch 129\n",
      "-------------------------------\n",
      "loss: 0.000565  [    0/  780]\n",
      "Test Error: \n",
      "  Avg loss: 0.000532 \n",
      "\n",
      "Epoch 130\n",
      "-------------------------------\n",
      "loss: 0.000519  [    0/  780]\n",
      "Test Error: \n",
      "  Avg loss: 0.001015 \n",
      "\n",
      "Epoch 131\n",
      "-------------------------------\n",
      "loss: 0.000625  [    0/  780]\n",
      "Test Error: \n",
      "  Avg loss: 0.000717 \n",
      "\n",
      "Epoch 132\n",
      "-------------------------------\n",
      "loss: 0.000525  [    0/  780]\n",
      "Test Error: \n",
      "  Avg loss: 0.000741 \n",
      "\n",
      "Epoch 133\n",
      "-------------------------------\n",
      "loss: 0.000536  [    0/  780]\n",
      "Test Error: \n",
      "  Avg loss: 0.000597 \n",
      "\n",
      "Epoch 134\n",
      "-------------------------------\n",
      "loss: 0.000422  [    0/  780]\n",
      "Test Error: \n",
      "  Avg loss: 0.000484 \n",
      "\n",
      "Epoch 135\n",
      "-------------------------------\n",
      "loss: 0.000383  [    0/  780]\n",
      "Test Error: \n",
      "  Avg loss: 0.000635 \n",
      "\n",
      "Epoch 136\n",
      "-------------------------------\n",
      "loss: 0.000479  [    0/  780]\n",
      "Test Error: \n",
      "  Avg loss: 0.000820 \n",
      "\n",
      "Epoch 137\n",
      "-------------------------------\n",
      "loss: 0.000343  [    0/  780]\n",
      "Test Error: \n",
      "  Avg loss: 0.000664 \n",
      "\n",
      "Epoch 138\n",
      "-------------------------------\n",
      "loss: 0.000516  [    0/  780]\n",
      "Test Error: \n",
      "  Avg loss: 0.000731 \n",
      "\n",
      "Epoch 139\n",
      "-------------------------------\n",
      "loss: 0.000592  [    0/  780]\n",
      "Test Error: \n",
      "  Avg loss: 0.000562 \n",
      "\n",
      "Epoch 140\n",
      "-------------------------------\n",
      "loss: 0.000427  [    0/  780]\n",
      "Test Error: \n",
      "  Avg loss: 0.000544 \n",
      "\n",
      "Epoch 141\n",
      "-------------------------------\n",
      "loss: 0.000345  [    0/  780]\n",
      "Test Error: \n",
      "  Avg loss: 0.000524 \n",
      "\n",
      "Epoch 142\n",
      "-------------------------------\n",
      "loss: 0.000322  [    0/  780]\n",
      "Test Error: \n",
      "  Avg loss: 0.000570 \n",
      "\n",
      "Epoch 143\n",
      "-------------------------------\n",
      "loss: 0.000386  [    0/  780]\n",
      "Test Error: \n",
      "  Avg loss: 0.000522 \n",
      "\n",
      "Epoch 144\n",
      "-------------------------------\n",
      "loss: 0.000490  [    0/  780]\n",
      "Test Error: \n",
      "  Avg loss: 0.000501 \n",
      "\n",
      "Epoch 145\n",
      "-------------------------------\n",
      "loss: 0.000329  [    0/  780]\n",
      "Test Error: \n",
      "  Avg loss: 0.000673 \n",
      "\n",
      "Epoch 146\n",
      "-------------------------------\n",
      "loss: 0.000400  [    0/  780]\n",
      "Test Error: \n",
      "  Avg loss: 0.000549 \n",
      "\n",
      "Epoch 147\n",
      "-------------------------------\n",
      "loss: 0.000391  [    0/  780]\n",
      "Test Error: \n",
      "  Avg loss: 0.000740 \n",
      "\n",
      "Epoch 148\n",
      "-------------------------------\n",
      "loss: 0.000553  [    0/  780]\n",
      "Test Error: \n",
      "  Avg loss: 0.000494 \n",
      "\n",
      "Epoch 149\n",
      "-------------------------------\n",
      "loss: 0.000525  [    0/  780]\n",
      "Test Error: \n",
      "  Avg loss: 0.000662 \n",
      "\n",
      "Epoch 150\n",
      "-------------------------------\n",
      "loss: 0.000502  [    0/  780]\n",
      "Test Error: \n",
      "  Avg loss: 0.000480 \n",
      "\n",
      "Epoch 151\n",
      "-------------------------------\n",
      "loss: 0.000503  [    0/  780]\n",
      "Test Error: \n",
      "  Avg loss: 0.000627 \n",
      "\n",
      "Epoch 152\n",
      "-------------------------------\n",
      "loss: 0.000482  [    0/  780]\n",
      "Test Error: \n",
      "  Avg loss: 0.000594 \n",
      "\n",
      "Epoch 153\n",
      "-------------------------------\n",
      "loss: 0.000346  [    0/  780]\n",
      "Test Error: \n",
      "  Avg loss: 0.000479 \n",
      "\n",
      "Epoch 154\n",
      "-------------------------------\n",
      "loss: 0.000415  [    0/  780]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test Error: \n",
      "  Avg loss: 0.000683 \n",
      "\n",
      "Epoch 155\n",
      "-------------------------------\n",
      "loss: 0.000419  [    0/  780]\n",
      "Test Error: \n",
      "  Avg loss: 0.000953 \n",
      "\n",
      "Epoch 156\n",
      "-------------------------------\n",
      "loss: 0.000412  [    0/  780]\n",
      "Test Error: \n",
      "  Avg loss: 0.000610 \n",
      "\n",
      "Epoch 157\n",
      "-------------------------------\n",
      "loss: 0.000566  [    0/  780]\n",
      "Test Error: \n",
      "  Avg loss: 0.000536 \n",
      "\n",
      "Epoch 158\n",
      "-------------------------------\n",
      "loss: 0.000319  [    0/  780]\n",
      "Test Error: \n",
      "  Avg loss: 0.000576 \n",
      "\n",
      "Epoch 159\n",
      "-------------------------------\n",
      "loss: 0.000380  [    0/  780]\n",
      "Test Error: \n",
      "  Avg loss: 0.000635 \n",
      "\n",
      "Epoch 160\n",
      "-------------------------------\n",
      "loss: 0.000399  [    0/  780]\n",
      "Test Error: \n",
      "  Avg loss: 0.000435 \n",
      "\n",
      "Epoch 161\n",
      "-------------------------------\n",
      "loss: 0.000309  [    0/  780]\n",
      "Test Error: \n",
      "  Avg loss: 0.000474 \n",
      "\n",
      "Epoch 162\n",
      "-------------------------------\n",
      "loss: 0.000296  [    0/  780]\n",
      "Test Error: \n",
      "  Avg loss: 0.000534 \n",
      "\n",
      "Epoch 163\n",
      "-------------------------------\n",
      "loss: 0.000317  [    0/  780]\n",
      "Test Error: \n",
      "  Avg loss: 0.000562 \n",
      "\n",
      "Epoch 164\n",
      "-------------------------------\n",
      "loss: 0.000352  [    0/  780]\n",
      "Test Error: \n",
      "  Avg loss: 0.000540 \n",
      "\n",
      "Epoch 165\n",
      "-------------------------------\n",
      "loss: 0.000337  [    0/  780]\n",
      "Test Error: \n",
      "  Avg loss: 0.000648 \n",
      "\n",
      "Epoch 166\n",
      "-------------------------------\n",
      "loss: 0.000535  [    0/  780]\n",
      "Test Error: \n",
      "  Avg loss: 0.000467 \n",
      "\n",
      "Epoch 167\n",
      "-------------------------------\n",
      "loss: 0.000275  [    0/  780]\n",
      "Test Error: \n",
      "  Avg loss: 0.000463 \n",
      "\n",
      "Epoch 168\n",
      "-------------------------------\n",
      "loss: 0.000489  [    0/  780]\n",
      "Test Error: \n",
      "  Avg loss: 0.000474 \n",
      "\n",
      "Epoch 169\n",
      "-------------------------------\n",
      "loss: 0.000320  [    0/  780]\n",
      "Test Error: \n",
      "  Avg loss: 0.000616 \n",
      "\n",
      "Epoch 170\n",
      "-------------------------------\n",
      "loss: 0.000381  [    0/  780]\n",
      "Test Error: \n",
      "  Avg loss: 0.000573 \n",
      "\n",
      "Epoch 171\n",
      "-------------------------------\n",
      "loss: 0.000367  [    0/  780]\n",
      "Test Error: \n",
      "  Avg loss: 0.000600 \n",
      "\n",
      "Epoch 172\n",
      "-------------------------------\n",
      "loss: 0.000472  [    0/  780]\n",
      "Test Error: \n",
      "  Avg loss: 0.000456 \n",
      "\n",
      "Epoch 173\n",
      "-------------------------------\n",
      "loss: 0.000308  [    0/  780]\n",
      "Test Error: \n",
      "  Avg loss: 0.000550 \n",
      "\n",
      "Epoch 174\n",
      "-------------------------------\n",
      "loss: 0.000273  [    0/  780]\n",
      "Test Error: \n",
      "  Avg loss: 0.000504 \n",
      "\n",
      "Epoch 175\n",
      "-------------------------------\n",
      "loss: 0.000452  [    0/  780]\n",
      "Test Error: \n",
      "  Avg loss: 0.000495 \n",
      "\n",
      "Epoch 176\n",
      "-------------------------------\n",
      "loss: 0.000366  [    0/  780]\n",
      "Test Error: \n",
      "  Avg loss: 0.000501 \n",
      "\n",
      "Epoch 177\n",
      "-------------------------------\n",
      "loss: 0.000391  [    0/  780]\n",
      "Test Error: \n",
      "  Avg loss: 0.000526 \n",
      "\n",
      "Epoch 178\n",
      "-------------------------------\n",
      "loss: 0.000335  [    0/  780]\n",
      "Test Error: \n",
      "  Avg loss: 0.000676 \n",
      "\n",
      "Epoch 179\n",
      "-------------------------------\n",
      "loss: 0.000485  [    0/  780]\n",
      "Test Error: \n",
      "  Avg loss: 0.000554 \n",
      "\n",
      "Epoch 180\n",
      "-------------------------------\n",
      "loss: 0.000462  [    0/  780]\n",
      "Test Error: \n",
      "  Avg loss: 0.000542 \n",
      "\n",
      "Epoch 181\n",
      "-------------------------------\n",
      "loss: 0.000380  [    0/  780]\n",
      "Test Error: \n",
      "  Avg loss: 0.000477 \n",
      "\n",
      "Epoch 182\n",
      "-------------------------------\n",
      "loss: 0.000463  [    0/  780]\n",
      "Test Error: \n",
      "  Avg loss: 0.000855 \n",
      "\n",
      "Epoch 183\n",
      "-------------------------------\n",
      "loss: 0.000569  [    0/  780]\n",
      "Test Error: \n",
      "  Avg loss: 0.000537 \n",
      "\n",
      "Epoch 184\n",
      "-------------------------------\n",
      "loss: 0.000500  [    0/  780]\n",
      "Test Error: \n",
      "  Avg loss: 0.000586 \n",
      "\n",
      "Epoch 185\n",
      "-------------------------------\n",
      "loss: 0.000298  [    0/  780]\n",
      "Test Error: \n",
      "  Avg loss: 0.000607 \n",
      "\n",
      "Epoch 186\n",
      "-------------------------------\n",
      "loss: 0.000324  [    0/  780]\n",
      "Test Error: \n",
      "  Avg loss: 0.000464 \n",
      "\n",
      "Epoch 187\n",
      "-------------------------------\n",
      "loss: 0.000280  [    0/  780]\n",
      "Test Error: \n",
      "  Avg loss: 0.000509 \n",
      "\n",
      "Epoch 188\n",
      "-------------------------------\n",
      "loss: 0.000300  [    0/  780]\n",
      "Test Error: \n",
      "  Avg loss: 0.000506 \n",
      "\n",
      "Epoch 189\n",
      "-------------------------------\n",
      "loss: 0.000312  [    0/  780]\n",
      "Test Error: \n",
      "  Avg loss: 0.000504 \n",
      "\n",
      "Epoch 190\n",
      "-------------------------------\n",
      "loss: 0.000367  [    0/  780]\n",
      "Test Error: \n",
      "  Avg loss: 0.000428 \n",
      "\n",
      "Epoch 191\n",
      "-------------------------------\n",
      "loss: 0.000300  [    0/  780]\n",
      "Test Error: \n",
      "  Avg loss: 0.000506 \n",
      "\n",
      "Epoch 192\n",
      "-------------------------------\n",
      "loss: 0.000354  [    0/  780]\n",
      "Test Error: \n",
      "  Avg loss: 0.000546 \n",
      "\n",
      "Epoch 193\n",
      "-------------------------------\n",
      "loss: 0.000375  [    0/  780]\n",
      "Test Error: \n",
      "  Avg loss: 0.000432 \n",
      "\n",
      "Epoch 194\n",
      "-------------------------------\n",
      "loss: 0.000240  [    0/  780]\n",
      "Test Error: \n",
      "  Avg loss: 0.000470 \n",
      "\n",
      "Epoch 195\n",
      "-------------------------------\n",
      "loss: 0.000284  [    0/  780]\n",
      "Test Error: \n",
      "  Avg loss: 0.000762 \n",
      "\n",
      "Epoch 196\n",
      "-------------------------------\n",
      "loss: 0.000530  [    0/  780]\n",
      "Test Error: \n",
      "  Avg loss: 0.000526 \n",
      "\n",
      "Epoch 197\n",
      "-------------------------------\n",
      "loss: 0.000471  [    0/  780]\n",
      "Test Error: \n",
      "  Avg loss: 0.000499 \n",
      "\n",
      "Epoch 198\n",
      "-------------------------------\n",
      "loss: 0.000383  [    0/  780]\n",
      "Test Error: \n",
      "  Avg loss: 0.000531 \n",
      "\n",
      "Epoch 199\n",
      "-------------------------------\n",
      "loss: 0.000591  [    0/  780]\n",
      "Test Error: \n",
      "  Avg loss: 0.000570 \n",
      "\n",
      "Epoch 200\n",
      "-------------------------------\n",
      "loss: 0.000485  [    0/  780]\n",
      "Test Error: \n",
      "  Avg loss: 0.000488 \n",
      "\n",
      "Epoch 201\n",
      "-------------------------------\n",
      "loss: 0.000479  [    0/  780]\n",
      "Test Error: \n",
      "  Avg loss: 0.000499 \n",
      "\n",
      "Epoch 202\n",
      "-------------------------------\n",
      "loss: 0.000369  [    0/  780]\n",
      "Test Error: \n",
      "  Avg loss: 0.000554 \n",
      "\n",
      "Epoch 203\n",
      "-------------------------------\n",
      "loss: 0.000341  [    0/  780]\n",
      "Test Error: \n",
      "  Avg loss: 0.000546 \n",
      "\n",
      "Epoch 204\n",
      "-------------------------------\n",
      "loss: 0.000397  [    0/  780]\n",
      "Test Error: \n",
      "  Avg loss: 0.000472 \n",
      "\n",
      "Epoch 205\n",
      "-------------------------------\n",
      "loss: 0.000346  [    0/  780]\n",
      "Test Error: \n",
      "  Avg loss: 0.000485 \n",
      "\n",
      "Epoch 206\n",
      "-------------------------------\n",
      "loss: 0.000465  [    0/  780]\n",
      "Test Error: \n",
      "  Avg loss: 0.000409 \n",
      "\n",
      "Epoch 207\n",
      "-------------------------------\n",
      "loss: 0.000286  [    0/  780]\n",
      "Test Error: \n",
      "  Avg loss: 0.000507 \n",
      "\n",
      "Epoch 208\n",
      "-------------------------------\n",
      "loss: 0.000370  [    0/  780]\n",
      "Test Error: \n",
      "  Avg loss: 0.000429 \n",
      "\n",
      "Epoch 209\n",
      "-------------------------------\n",
      "loss: 0.000376  [    0/  780]\n",
      "Test Error: \n",
      "  Avg loss: 0.000395 \n",
      "\n",
      "Epoch 210\n",
      "-------------------------------\n",
      "loss: 0.000490  [    0/  780]\n",
      "Test Error: \n",
      "  Avg loss: 0.000557 \n",
      "\n",
      "Epoch 211\n",
      "-------------------------------\n",
      "loss: 0.000359  [    0/  780]\n",
      "Test Error: \n",
      "  Avg loss: 0.000527 \n",
      "\n",
      "Epoch 212\n",
      "-------------------------------\n",
      "loss: 0.000369  [    0/  780]\n",
      "Test Error: \n",
      "  Avg loss: 0.000564 \n",
      "\n",
      "Epoch 213\n",
      "-------------------------------\n",
      "loss: 0.000338  [    0/  780]\n",
      "Test Error: \n",
      "  Avg loss: 0.000422 \n",
      "\n",
      "Epoch 214\n",
      "-------------------------------\n",
      "loss: 0.000306  [    0/  780]\n",
      "Test Error: \n",
      "  Avg loss: 0.000377 \n",
      "\n",
      "Epoch 215\n",
      "-------------------------------\n",
      "loss: 0.000272  [    0/  780]\n",
      "Test Error: \n",
      "  Avg loss: 0.000500 \n",
      "\n",
      "Epoch 216\n",
      "-------------------------------\n",
      "loss: 0.000341  [    0/  780]\n",
      "Test Error: \n",
      "  Avg loss: 0.000504 \n",
      "\n",
      "Epoch 217\n",
      "-------------------------------\n",
      "loss: 0.000363  [    0/  780]\n",
      "Test Error: \n",
      "  Avg loss: 0.000482 \n",
      "\n",
      "Epoch 218\n",
      "-------------------------------\n",
      "loss: 0.000312  [    0/  780]\n",
      "Test Error: \n",
      "  Avg loss: 0.000603 \n",
      "\n",
      "Epoch 219\n",
      "-------------------------------\n",
      "loss: 0.000373  [    0/  780]\n",
      "Test Error: \n",
      "  Avg loss: 0.000490 \n",
      "\n",
      "Epoch 220\n",
      "-------------------------------\n",
      "loss: 0.000404  [    0/  780]\n",
      "Test Error: \n",
      "  Avg loss: 0.000388 \n",
      "\n",
      "Epoch 221\n",
      "-------------------------------\n",
      "loss: 0.000298  [    0/  780]\n",
      "Test Error: \n",
      "  Avg loss: 0.000683 \n",
      "\n",
      "Epoch 222\n",
      "-------------------------------\n",
      "loss: 0.000304  [    0/  780]\n",
      "Test Error: \n",
      "  Avg loss: 0.000530 \n",
      "\n",
      "Epoch 223\n",
      "-------------------------------\n",
      "loss: 0.000299  [    0/  780]\n",
      "Test Error: \n",
      "  Avg loss: 0.000482 \n",
      "\n",
      "Epoch 224\n",
      "-------------------------------\n",
      "loss: 0.000371  [    0/  780]\n",
      "Test Error: \n",
      "  Avg loss: 0.000414 \n",
      "\n",
      "Epoch 225\n",
      "-------------------------------\n",
      "loss: 0.000362  [    0/  780]\n",
      "Test Error: \n",
      "  Avg loss: 0.000642 \n",
      "\n",
      "Epoch 226\n",
      "-------------------------------\n",
      "loss: 0.000447  [    0/  780]\n",
      "Test Error: \n",
      "  Avg loss: 0.000584 \n",
      "\n",
      "Epoch 227\n",
      "-------------------------------\n",
      "loss: 0.000469  [    0/  780]\n",
      "Test Error: \n",
      "  Avg loss: 0.000461 \n",
      "\n",
      "Epoch 228\n",
      "-------------------------------\n",
      "loss: 0.000437  [    0/  780]\n",
      "Test Error: \n",
      "  Avg loss: 0.000600 \n",
      "\n",
      "Epoch 229\n",
      "-------------------------------\n",
      "loss: 0.000394  [    0/  780]\n",
      "Test Error: \n",
      "  Avg loss: 0.000480 \n",
      "\n",
      "Epoch 230\n",
      "-------------------------------\n",
      "loss: 0.000227  [    0/  780]\n",
      "Test Error: \n",
      "  Avg loss: 0.000615 \n",
      "\n",
      "Epoch 231\n",
      "-------------------------------\n",
      "loss: 0.000424  [    0/  780]\n",
      "Test Error: \n",
      "  Avg loss: 0.000440 \n",
      "\n",
      "Epoch 232\n",
      "-------------------------------\n",
      "loss: 0.000398  [    0/  780]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test Error: \n",
      "  Avg loss: 0.000389 \n",
      "\n",
      "Epoch 233\n",
      "-------------------------------\n",
      "loss: 0.000205  [    0/  780]\n",
      "Test Error: \n",
      "  Avg loss: 0.000594 \n",
      "\n",
      "Epoch 234\n",
      "-------------------------------\n",
      "loss: 0.000305  [    0/  780]\n",
      "Test Error: \n",
      "  Avg loss: 0.000471 \n",
      "\n",
      "Epoch 235\n",
      "-------------------------------\n",
      "loss: 0.000396  [    0/  780]\n",
      "Test Error: \n",
      "  Avg loss: 0.000466 \n",
      "\n",
      "Epoch 236\n",
      "-------------------------------\n",
      "loss: 0.000450  [    0/  780]\n",
      "Test Error: \n",
      "  Avg loss: 0.000403 \n",
      "\n",
      "Epoch 237\n",
      "-------------------------------\n",
      "loss: 0.000270  [    0/  780]\n",
      "Test Error: \n",
      "  Avg loss: 0.000414 \n",
      "\n",
      "Epoch 238\n",
      "-------------------------------\n",
      "loss: 0.000269  [    0/  780]\n",
      "Test Error: \n",
      "  Avg loss: 0.000491 \n",
      "\n",
      "Epoch 239\n",
      "-------------------------------\n",
      "loss: 0.000313  [    0/  780]\n",
      "Test Error: \n",
      "  Avg loss: 0.000516 \n",
      "\n",
      "Epoch 240\n",
      "-------------------------------\n",
      "loss: 0.000353  [    0/  780]\n",
      "Test Error: \n",
      "  Avg loss: 0.000646 \n",
      "\n",
      "Epoch 241\n",
      "-------------------------------\n",
      "loss: 0.000376  [    0/  780]\n",
      "Test Error: \n",
      "  Avg loss: 0.000418 \n",
      "\n",
      "Epoch 242\n",
      "-------------------------------\n",
      "loss: 0.000294  [    0/  780]\n",
      "Test Error: \n",
      "  Avg loss: 0.000398 \n",
      "\n",
      "Epoch 243\n",
      "-------------------------------\n",
      "loss: 0.000251  [    0/  780]\n",
      "Test Error: \n",
      "  Avg loss: 0.000432 \n",
      "\n",
      "Epoch 244\n",
      "-------------------------------\n",
      "loss: 0.000228  [    0/  780]\n",
      "Test Error: \n",
      "  Avg loss: 0.000620 \n",
      "\n",
      "Epoch 245\n",
      "-------------------------------\n",
      "loss: 0.000364  [    0/  780]\n",
      "Test Error: \n",
      "  Avg loss: 0.000473 \n",
      "\n",
      "Epoch 246\n",
      "-------------------------------\n",
      "loss: 0.000328  [    0/  780]\n",
      "Test Error: \n",
      "  Avg loss: 0.000385 \n",
      "\n",
      "Epoch 247\n",
      "-------------------------------\n",
      "loss: 0.000287  [    0/  780]\n",
      "Test Error: \n",
      "  Avg loss: 0.000526 \n",
      "\n",
      "Epoch 248\n",
      "-------------------------------\n",
      "loss: 0.000532  [    0/  780]\n",
      "Test Error: \n",
      "  Avg loss: 0.000513 \n",
      "\n",
      "Epoch 249\n",
      "-------------------------------\n",
      "loss: 0.000304  [    0/  780]\n",
      "Test Error: \n",
      "  Avg loss: 0.000494 \n",
      "\n",
      "Epoch 250\n",
      "-------------------------------\n",
      "loss: 0.000326  [    0/  780]\n",
      "Test Error: \n",
      "  Avg loss: 0.000529 \n",
      "\n",
      "Epoch 251\n",
      "-------------------------------\n",
      "loss: 0.000310  [    0/  780]\n",
      "Test Error: \n",
      "  Avg loss: 0.000570 \n",
      "\n",
      "Epoch 252\n",
      "-------------------------------\n",
      "loss: 0.000556  [    0/  780]\n",
      "Test Error: \n",
      "  Avg loss: 0.000488 \n",
      "\n",
      "Epoch 253\n",
      "-------------------------------\n",
      "loss: 0.000310  [    0/  780]\n",
      "Test Error: \n",
      "  Avg loss: 0.000524 \n",
      "\n",
      "Epoch 254\n",
      "-------------------------------\n",
      "loss: 0.000385  [    0/  780]\n",
      "Test Error: \n",
      "  Avg loss: 0.000411 \n",
      "\n",
      "Epoch 255\n",
      "-------------------------------\n",
      "loss: 0.000216  [    0/  780]\n",
      "Test Error: \n",
      "  Avg loss: 0.000454 \n",
      "\n",
      "Epoch 256\n",
      "-------------------------------\n",
      "loss: 0.000187  [    0/  780]\n",
      "Test Error: \n",
      "  Avg loss: 0.000449 \n",
      "\n",
      "Epoch 257\n",
      "-------------------------------\n",
      "loss: 0.000279  [    0/  780]\n",
      "Test Error: \n",
      "  Avg loss: 0.000499 \n",
      "\n",
      "Epoch 258\n",
      "-------------------------------\n",
      "loss: 0.000382  [    0/  780]\n",
      "Test Error: \n",
      "  Avg loss: 0.000406 \n",
      "\n",
      "Epoch 259\n",
      "-------------------------------\n",
      "loss: 0.000342  [    0/  780]\n",
      "Test Error: \n",
      "  Avg loss: 0.000414 \n",
      "\n",
      "Epoch 260\n",
      "-------------------------------\n",
      "loss: 0.000181  [    0/  780]\n",
      "Test Error: \n",
      "  Avg loss: 0.000405 \n",
      "\n",
      "Epoch 261\n",
      "-------------------------------\n",
      "loss: 0.000179  [    0/  780]\n",
      "Test Error: \n",
      "  Avg loss: 0.000649 \n",
      "\n",
      "Epoch 262\n",
      "-------------------------------\n",
      "loss: 0.000350  [    0/  780]\n",
      "Test Error: \n",
      "  Avg loss: 0.000501 \n",
      "\n",
      "Epoch 263\n",
      "-------------------------------\n",
      "loss: 0.000321  [    0/  780]\n",
      "Test Error: \n",
      "  Avg loss: 0.000460 \n",
      "\n",
      "Epoch 264\n",
      "-------------------------------\n",
      "loss: 0.000304  [    0/  780]\n",
      "Test Error: \n",
      "  Avg loss: 0.000387 \n",
      "\n",
      "Epoch 265\n",
      "-------------------------------\n",
      "loss: 0.000260  [    0/  780]\n",
      "Test Error: \n",
      "  Avg loss: 0.000413 \n",
      "\n",
      "Epoch 266\n",
      "-------------------------------\n",
      "loss: 0.000265  [    0/  780]\n",
      "Test Error: \n",
      "  Avg loss: 0.000385 \n",
      "\n",
      "Epoch 267\n",
      "-------------------------------\n",
      "loss: 0.000283  [    0/  780]\n",
      "Test Error: \n",
      "  Avg loss: 0.000628 \n",
      "\n",
      "Epoch 268\n",
      "-------------------------------\n",
      "loss: 0.000243  [    0/  780]\n",
      "Test Error: \n",
      "  Avg loss: 0.000477 \n",
      "\n",
      "Epoch 269\n",
      "-------------------------------\n",
      "loss: 0.000261  [    0/  780]\n",
      "Test Error: \n",
      "  Avg loss: 0.000529 \n",
      "\n",
      "Epoch 270\n",
      "-------------------------------\n",
      "loss: 0.000235  [    0/  780]\n",
      "Test Error: \n",
      "  Avg loss: 0.000457 \n",
      "\n",
      "Epoch 271\n",
      "-------------------------------\n",
      "loss: 0.000354  [    0/  780]\n",
      "Test Error: \n",
      "  Avg loss: 0.000596 \n",
      "\n",
      "Epoch 272\n",
      "-------------------------------\n",
      "loss: 0.000369  [    0/  780]\n",
      "Test Error: \n",
      "  Avg loss: 0.000709 \n",
      "\n",
      "Epoch 273\n",
      "-------------------------------\n",
      "loss: 0.000352  [    0/  780]\n",
      "Test Error: \n",
      "  Avg loss: 0.000480 \n",
      "\n",
      "Epoch 274\n",
      "-------------------------------\n",
      "loss: 0.000414  [    0/  780]\n",
      "Test Error: \n",
      "  Avg loss: 0.000457 \n",
      "\n",
      "Epoch 275\n",
      "-------------------------------\n",
      "loss: 0.000319  [    0/  780]\n",
      "Test Error: \n",
      "  Avg loss: 0.000635 \n",
      "\n",
      "Epoch 276\n",
      "-------------------------------\n",
      "loss: 0.000458  [    0/  780]\n",
      "Test Error: \n",
      "  Avg loss: 0.000405 \n",
      "\n",
      "Epoch 277\n",
      "-------------------------------\n",
      "loss: 0.000288  [    0/  780]\n",
      "Test Error: \n",
      "  Avg loss: 0.000393 \n",
      "\n",
      "Epoch 278\n",
      "-------------------------------\n",
      "loss: 0.000205  [    0/  780]\n",
      "Test Error: \n",
      "  Avg loss: 0.000488 \n",
      "\n",
      "Epoch 279\n",
      "-------------------------------\n",
      "loss: 0.000230  [    0/  780]\n",
      "Test Error: \n",
      "  Avg loss: 0.000463 \n",
      "\n",
      "Epoch 280\n",
      "-------------------------------\n",
      "loss: 0.000387  [    0/  780]\n",
      "Test Error: \n",
      "  Avg loss: 0.000435 \n",
      "\n",
      "Epoch 281\n",
      "-------------------------------\n",
      "loss: 0.000262  [    0/  780]\n",
      "Test Error: \n",
      "  Avg loss: 0.000501 \n",
      "\n",
      "Epoch 282\n",
      "-------------------------------\n",
      "loss: 0.000271  [    0/  780]\n",
      "Test Error: \n",
      "  Avg loss: 0.000521 \n",
      "\n",
      "Epoch 283\n",
      "-------------------------------\n",
      "loss: 0.000164  [    0/  780]\n",
      "Test Error: \n",
      "  Avg loss: 0.000418 \n",
      "\n",
      "Epoch 284\n",
      "-------------------------------\n",
      "loss: 0.000368  [    0/  780]\n",
      "Test Error: \n",
      "  Avg loss: 0.000470 \n",
      "\n",
      "Epoch 285\n",
      "-------------------------------\n",
      "loss: 0.000269  [    0/  780]\n",
      "Test Error: \n",
      "  Avg loss: 0.000485 \n",
      "\n",
      "Epoch 286\n",
      "-------------------------------\n",
      "loss: 0.000276  [    0/  780]\n",
      "Test Error: \n",
      "  Avg loss: 0.000419 \n",
      "\n",
      "Epoch 287\n",
      "-------------------------------\n",
      "loss: 0.000200  [    0/  780]\n",
      "Test Error: \n",
      "  Avg loss: 0.000487 \n",
      "\n",
      "Epoch 288\n",
      "-------------------------------\n",
      "loss: 0.000296  [    0/  780]\n",
      "Test Error: \n",
      "  Avg loss: 0.000401 \n",
      "\n",
      "Epoch 289\n",
      "-------------------------------\n",
      "loss: 0.000274  [    0/  780]\n",
      "Test Error: \n",
      "  Avg loss: 0.000462 \n",
      "\n",
      "Epoch 290\n",
      "-------------------------------\n",
      "loss: 0.000322  [    0/  780]\n",
      "Test Error: \n",
      "  Avg loss: 0.000444 \n",
      "\n",
      "Epoch 291\n",
      "-------------------------------\n",
      "loss: 0.000220  [    0/  780]\n",
      "Test Error: \n",
      "  Avg loss: 0.000490 \n",
      "\n",
      "Epoch 292\n",
      "-------------------------------\n",
      "loss: 0.000302  [    0/  780]\n",
      "Test Error: \n",
      "  Avg loss: 0.000537 \n",
      "\n",
      "Epoch 293\n",
      "-------------------------------\n",
      "loss: 0.000310  [    0/  780]\n",
      "Test Error: \n",
      "  Avg loss: 0.000545 \n",
      "\n",
      "Epoch 294\n",
      "-------------------------------\n",
      "loss: 0.000299  [    0/  780]\n",
      "Test Error: \n",
      "  Avg loss: 0.000479 \n",
      "\n",
      "Epoch 295\n",
      "-------------------------------\n",
      "loss: 0.000403  [    0/  780]\n",
      "Test Error: \n",
      "  Avg loss: 0.000438 \n",
      "\n",
      "Epoch 296\n",
      "-------------------------------\n",
      "loss: 0.000261  [    0/  780]\n",
      "Test Error: \n",
      "  Avg loss: 0.000489 \n",
      "\n",
      "Epoch 297\n",
      "-------------------------------\n",
      "loss: 0.000222  [    0/  780]\n",
      "Test Error: \n",
      "  Avg loss: 0.000401 \n",
      "\n",
      "Epoch 298\n",
      "-------------------------------\n",
      "loss: 0.000303  [    0/  780]\n",
      "Test Error: \n",
      "  Avg loss: 0.000451 \n",
      "\n",
      "Epoch 299\n",
      "-------------------------------\n",
      "loss: 0.000195  [    0/  780]\n",
      "Test Error: \n",
      "  Avg loss: 0.000463 \n",
      "\n",
      "Epoch 300\n",
      "-------------------------------\n",
      "loss: 0.000254  [    0/  780]\n",
      "Test Error: \n",
      "  Avg loss: 0.000396 \n",
      "\n",
      "Epoch 301\n",
      "-------------------------------\n",
      "loss: 0.000395  [    0/  780]\n",
      "Test Error: \n",
      "  Avg loss: 0.000706 \n",
      "\n",
      "Epoch 302\n",
      "-------------------------------\n",
      "loss: 0.000301  [    0/  780]\n",
      "Test Error: \n",
      "  Avg loss: 0.000534 \n",
      "\n",
      "Epoch 303\n",
      "-------------------------------\n",
      "loss: 0.000283  [    0/  780]\n",
      "Test Error: \n",
      "  Avg loss: 0.000457 \n",
      "\n",
      "Epoch 304\n",
      "-------------------------------\n",
      "loss: 0.000215  [    0/  780]\n",
      "Test Error: \n",
      "  Avg loss: 0.000403 \n",
      "\n",
      "Epoch 305\n",
      "-------------------------------\n",
      "loss: 0.000342  [    0/  780]\n",
      "Test Error: \n",
      "  Avg loss: 0.000596 \n",
      "\n",
      "Epoch 306\n",
      "-------------------------------\n",
      "loss: 0.000284  [    0/  780]\n",
      "Test Error: \n",
      "  Avg loss: 0.000389 \n",
      "\n",
      "Epoch 307\n",
      "-------------------------------\n",
      "loss: 0.000199  [    0/  780]\n",
      "Test Error: \n",
      "  Avg loss: 0.000428 \n",
      "\n",
      "Epoch 308\n",
      "-------------------------------\n",
      "loss: 0.000204  [    0/  780]\n",
      "Test Error: \n",
      "  Avg loss: 0.000446 \n",
      "\n",
      "Epoch 309\n",
      "-------------------------------\n",
      "loss: 0.000196  [    0/  780]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test Error: \n",
      "  Avg loss: 0.000424 \n",
      "\n",
      "Epoch 310\n",
      "-------------------------------\n",
      "loss: 0.000245  [    0/  780]\n",
      "Test Error: \n",
      "  Avg loss: 0.000492 \n",
      "\n",
      "Epoch 311\n",
      "-------------------------------\n",
      "loss: 0.000182  [    0/  780]\n",
      "Test Error: \n",
      "  Avg loss: 0.000424 \n",
      "\n",
      "Epoch 312\n",
      "-------------------------------\n",
      "loss: 0.000193  [    0/  780]\n",
      "Test Error: \n",
      "  Avg loss: 0.000627 \n",
      "\n",
      "Epoch 313\n",
      "-------------------------------\n",
      "loss: 0.000336  [    0/  780]\n",
      "Test Error: \n",
      "  Avg loss: 0.000407 \n",
      "\n",
      "Epoch 314\n",
      "-------------------------------\n",
      "loss: 0.000287  [    0/  780]\n",
      "Test Error: \n",
      "  Avg loss: 0.000386 \n",
      "\n",
      "Epoch 315\n",
      "-------------------------------\n",
      "loss: 0.000154  [    0/  780]\n",
      "Test Error: \n",
      "  Avg loss: 0.000683 \n",
      "\n",
      "Epoch 316\n",
      "-------------------------------\n",
      "loss: 0.000466  [    0/  780]\n",
      "Test Error: \n",
      "  Avg loss: 0.000398 \n",
      "\n",
      "Epoch 317\n",
      "-------------------------------\n",
      "loss: 0.000245  [    0/  780]\n",
      "Test Error: \n",
      "  Avg loss: 0.000394 \n",
      "\n",
      "Epoch 318\n",
      "-------------------------------\n",
      "loss: 0.000203  [    0/  780]\n",
      "Test Error: \n",
      "  Avg loss: 0.000407 \n",
      "\n",
      "Epoch 319\n",
      "-------------------------------\n",
      "loss: 0.000188  [    0/  780]\n",
      "Test Error: \n",
      "  Avg loss: 0.000453 \n",
      "\n",
      "Epoch 320\n",
      "-------------------------------\n",
      "loss: 0.000138  [    0/  780]\n",
      "Test Error: \n",
      "  Avg loss: 0.000414 \n",
      "\n",
      "Epoch 321\n",
      "-------------------------------\n",
      "loss: 0.000246  [    0/  780]\n",
      "Test Error: \n",
      "  Avg loss: 0.000489 \n",
      "\n",
      "Epoch 322\n",
      "-------------------------------\n",
      "loss: 0.000232  [    0/  780]\n",
      "Test Error: \n",
      "  Avg loss: 0.000519 \n",
      "\n",
      "Epoch 323\n",
      "-------------------------------\n",
      "loss: 0.000302  [    0/  780]\n",
      "Test Error: \n",
      "  Avg loss: 0.000469 \n",
      "\n",
      "Epoch 324\n",
      "-------------------------------\n",
      "loss: 0.000329  [    0/  780]\n",
      "Test Error: \n",
      "  Avg loss: 0.000467 \n",
      "\n",
      "Epoch 325\n",
      "-------------------------------\n",
      "loss: 0.000190  [    0/  780]\n",
      "Test Error: \n",
      "  Avg loss: 0.000470 \n",
      "\n",
      "Epoch 326\n",
      "-------------------------------\n",
      "loss: 0.000364  [    0/  780]\n",
      "Test Error: \n",
      "  Avg loss: 0.000437 \n",
      "\n",
      "Epoch 327\n",
      "-------------------------------\n",
      "loss: 0.000221  [    0/  780]\n",
      "Test Error: \n",
      "  Avg loss: 0.000455 \n",
      "\n",
      "Epoch 328\n",
      "-------------------------------\n",
      "loss: 0.000219  [    0/  780]\n",
      "Test Error: \n",
      "  Avg loss: 0.000388 \n",
      "\n",
      "Epoch 329\n",
      "-------------------------------\n",
      "loss: 0.000173  [    0/  780]\n",
      "Test Error: \n",
      "  Avg loss: 0.000385 \n",
      "\n",
      "Epoch 330\n",
      "-------------------------------\n",
      "loss: 0.000205  [    0/  780]\n",
      "Test Error: \n",
      "  Avg loss: 0.000433 \n",
      "\n",
      "Epoch 331\n",
      "-------------------------------\n",
      "loss: 0.000208  [    0/  780]\n",
      "Test Error: \n",
      "  Avg loss: 0.000408 \n",
      "\n",
      "Epoch 332\n",
      "-------------------------------\n",
      "loss: 0.000242  [    0/  780]\n",
      "Test Error: \n",
      "  Avg loss: 0.000441 \n",
      "\n",
      "Epoch 333\n",
      "-------------------------------\n",
      "loss: 0.000198  [    0/  780]\n",
      "Test Error: \n",
      "  Avg loss: 0.000429 \n",
      "\n",
      "Epoch 334\n",
      "-------------------------------\n",
      "loss: 0.000220  [    0/  780]\n",
      "Test Error: \n",
      "  Avg loss: 0.000468 \n",
      "\n",
      "Epoch 335\n",
      "-------------------------------\n",
      "loss: 0.000359  [    0/  780]\n",
      "Test Error: \n",
      "  Avg loss: 0.000787 \n",
      "\n",
      "Epoch 336\n",
      "-------------------------------\n",
      "loss: 0.000489  [    0/  780]\n",
      "Test Error: \n",
      "  Avg loss: 0.000457 \n",
      "\n",
      "Epoch 337\n",
      "-------------------------------\n",
      "loss: 0.000396  [    0/  780]\n",
      "Test Error: \n",
      "  Avg loss: 0.000398 \n",
      "\n",
      "Epoch 338\n",
      "-------------------------------\n",
      "loss: 0.000204  [    0/  780]\n",
      "Test Error: \n",
      "  Avg loss: 0.000344 \n",
      "\n",
      "Epoch 339\n",
      "-------------------------------\n",
      "loss: 0.000151  [    0/  780]\n",
      "Test Error: \n",
      "  Avg loss: 0.000402 \n",
      "\n",
      "Epoch 340\n",
      "-------------------------------\n",
      "loss: 0.000299  [    0/  780]\n",
      "Test Error: \n",
      "  Avg loss: 0.000555 \n",
      "\n",
      "Epoch 341\n",
      "-------------------------------\n",
      "loss: 0.000271  [    0/  780]\n",
      "Test Error: \n",
      "  Avg loss: 0.000474 \n",
      "\n",
      "Epoch 342\n",
      "-------------------------------\n",
      "loss: 0.000275  [    0/  780]\n",
      "Test Error: \n",
      "  Avg loss: 0.000430 \n",
      "\n",
      "Epoch 343\n",
      "-------------------------------\n",
      "loss: 0.000297  [    0/  780]\n",
      "Test Error: \n",
      "  Avg loss: 0.000408 \n",
      "\n",
      "Epoch 344\n",
      "-------------------------------\n",
      "loss: 0.000221  [    0/  780]\n",
      "Test Error: \n",
      "  Avg loss: 0.000525 \n",
      "\n",
      "Epoch 345\n",
      "-------------------------------\n",
      "loss: 0.000246  [    0/  780]\n",
      "Test Error: \n",
      "  Avg loss: 0.000339 \n",
      "\n",
      "Epoch 346\n",
      "-------------------------------\n",
      "loss: 0.000194  [    0/  780]\n",
      "Test Error: \n",
      "  Avg loss: 0.000499 \n",
      "\n",
      "Epoch 347\n",
      "-------------------------------\n",
      "loss: 0.000287  [    0/  780]\n",
      "Test Error: \n",
      "  Avg loss: 0.000410 \n",
      "\n",
      "Epoch 348\n",
      "-------------------------------\n",
      "loss: 0.000221  [    0/  780]\n",
      "Test Error: \n",
      "  Avg loss: 0.000412 \n",
      "\n",
      "Epoch 349\n",
      "-------------------------------\n",
      "loss: 0.000157  [    0/  780]\n",
      "Test Error: \n",
      "  Avg loss: 0.000429 \n",
      "\n",
      "Epoch 350\n",
      "-------------------------------\n",
      "loss: 0.000302  [    0/  780]\n",
      "Test Error: \n",
      "  Avg loss: 0.000412 \n",
      "\n",
      "Epoch 351\n",
      "-------------------------------\n",
      "loss: 0.000165  [    0/  780]\n",
      "Test Error: \n",
      "  Avg loss: 0.000471 \n",
      "\n",
      "Epoch 352\n",
      "-------------------------------\n",
      "loss: 0.000229  [    0/  780]\n",
      "Test Error: \n",
      "  Avg loss: 0.000428 \n",
      "\n",
      "Epoch 353\n",
      "-------------------------------\n",
      "loss: 0.000232  [    0/  780]\n",
      "Test Error: \n",
      "  Avg loss: 0.000430 \n",
      "\n",
      "Epoch 354\n",
      "-------------------------------\n",
      "loss: 0.000192  [    0/  780]\n",
      "Test Error: \n",
      "  Avg loss: 0.000381 \n",
      "\n",
      "Epoch 355\n",
      "-------------------------------\n",
      "loss: 0.000230  [    0/  780]\n",
      "Test Error: \n",
      "  Avg loss: 0.000396 \n",
      "\n",
      "Epoch 356\n",
      "-------------------------------\n",
      "loss: 0.000228  [    0/  780]\n",
      "Test Error: \n",
      "  Avg loss: 0.000478 \n",
      "\n",
      "Epoch 357\n",
      "-------------------------------\n",
      "loss: 0.000207  [    0/  780]\n",
      "Test Error: \n",
      "  Avg loss: 0.000496 \n",
      "\n",
      "Epoch 358\n",
      "-------------------------------\n",
      "loss: 0.000301  [    0/  780]\n",
      "Test Error: \n",
      "  Avg loss: 0.000396 \n",
      "\n",
      "Epoch 359\n",
      "-------------------------------\n",
      "loss: 0.000152  [    0/  780]\n",
      "Test Error: \n",
      "  Avg loss: 0.000551 \n",
      "\n",
      "Epoch 360\n",
      "-------------------------------\n",
      "loss: 0.000162  [    0/  780]\n",
      "Test Error: \n",
      "  Avg loss: 0.000344 \n",
      "\n",
      "Epoch 361\n",
      "-------------------------------\n",
      "loss: 0.000282  [    0/  780]\n",
      "Test Error: \n",
      "  Avg loss: 0.000533 \n",
      "\n",
      "Epoch 362\n",
      "-------------------------------\n",
      "loss: 0.000307  [    0/  780]\n",
      "Test Error: \n",
      "  Avg loss: 0.000411 \n",
      "\n",
      "Epoch 363\n",
      "-------------------------------\n",
      "loss: 0.000220  [    0/  780]\n",
      "Test Error: \n",
      "  Avg loss: 0.000424 \n",
      "\n",
      "Epoch 364\n",
      "-------------------------------\n",
      "loss: 0.000241  [    0/  780]\n",
      "Test Error: \n",
      "  Avg loss: 0.000672 \n",
      "\n",
      "Epoch 365\n",
      "-------------------------------\n",
      "loss: 0.000317  [    0/  780]\n",
      "Test Error: \n",
      "  Avg loss: 0.000432 \n",
      "\n",
      "Epoch 366\n",
      "-------------------------------\n",
      "loss: 0.000203  [    0/  780]\n",
      "Test Error: \n",
      "  Avg loss: 0.000458 \n",
      "\n",
      "Epoch 367\n",
      "-------------------------------\n",
      "loss: 0.000187  [    0/  780]\n",
      "Test Error: \n",
      "  Avg loss: 0.000364 \n",
      "\n",
      "Epoch 368\n",
      "-------------------------------\n",
      "loss: 0.000247  [    0/  780]\n",
      "Test Error: \n",
      "  Avg loss: 0.000344 \n",
      "\n",
      "Epoch 369\n",
      "-------------------------------\n",
      "loss: 0.000238  [    0/  780]\n",
      "Test Error: \n",
      "  Avg loss: 0.000413 \n",
      "\n",
      "Epoch 370\n",
      "-------------------------------\n",
      "loss: 0.000231  [    0/  780]\n",
      "Test Error: \n",
      "  Avg loss: 0.000473 \n",
      "\n",
      "Epoch 371\n",
      "-------------------------------\n",
      "loss: 0.000437  [    0/  780]\n",
      "Test Error: \n",
      "  Avg loss: 0.000451 \n",
      "\n",
      "Epoch 372\n",
      "-------------------------------\n",
      "loss: 0.000442  [    0/  780]\n",
      "Test Error: \n",
      "  Avg loss: 0.000496 \n",
      "\n",
      "Epoch 373\n",
      "-------------------------------\n",
      "loss: 0.000274  [    0/  780]\n",
      "Test Error: \n",
      "  Avg loss: 0.000376 \n",
      "\n",
      "Epoch 374\n",
      "-------------------------------\n",
      "loss: 0.000136  [    0/  780]\n",
      "Test Error: \n",
      "  Avg loss: 0.000438 \n",
      "\n",
      "Epoch 375\n",
      "-------------------------------\n",
      "loss: 0.000157  [    0/  780]\n",
      "Test Error: \n",
      "  Avg loss: 0.000386 \n",
      "\n",
      "Epoch 376\n",
      "-------------------------------\n",
      "loss: 0.000323  [    0/  780]\n",
      "Test Error: \n",
      "  Avg loss: 0.000503 \n",
      "\n",
      "Epoch 377\n",
      "-------------------------------\n",
      "loss: 0.000246  [    0/  780]\n",
      "Test Error: \n",
      "  Avg loss: 0.000407 \n",
      "\n",
      "Epoch 378\n",
      "-------------------------------\n",
      "loss: 0.000210  [    0/  780]\n",
      "Test Error: \n",
      "  Avg loss: 0.000396 \n",
      "\n",
      "Epoch 379\n",
      "-------------------------------\n",
      "loss: 0.000195  [    0/  780]\n",
      "Test Error: \n",
      "  Avg loss: 0.000413 \n",
      "\n",
      "Epoch 380\n",
      "-------------------------------\n",
      "loss: 0.000156  [    0/  780]\n",
      "Test Error: \n",
      "  Avg loss: 0.000643 \n",
      "\n",
      "Epoch 381\n",
      "-------------------------------\n",
      "loss: 0.000368  [    0/  780]\n",
      "Test Error: \n",
      "  Avg loss: 0.000489 \n",
      "\n",
      "Epoch 382\n",
      "-------------------------------\n",
      "loss: 0.000243  [    0/  780]\n",
      "Test Error: \n",
      "  Avg loss: 0.000388 \n",
      "\n",
      "Epoch 383\n",
      "-------------------------------\n",
      "loss: 0.000187  [    0/  780]\n",
      "Test Error: \n",
      "  Avg loss: 0.000332 \n",
      "\n",
      "Epoch 384\n",
      "-------------------------------\n",
      "loss: 0.000165  [    0/  780]\n",
      "Test Error: \n",
      "  Avg loss: 0.000574 \n",
      "\n",
      "Epoch 385\n",
      "-------------------------------\n",
      "loss: 0.000182  [    0/  780]\n",
      "Test Error: \n",
      "  Avg loss: 0.000325 \n",
      "\n",
      "Epoch 386\n",
      "-------------------------------\n",
      "loss: 0.000198  [    0/  780]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test Error: \n",
      "  Avg loss: 0.000408 \n",
      "\n",
      "Epoch 387\n",
      "-------------------------------\n",
      "loss: 0.000274  [    0/  780]\n",
      "Test Error: \n",
      "  Avg loss: 0.000635 \n",
      "\n",
      "Epoch 388\n",
      "-------------------------------\n",
      "loss: 0.000282  [    0/  780]\n",
      "Test Error: \n",
      "  Avg loss: 0.000476 \n",
      "\n",
      "Epoch 389\n",
      "-------------------------------\n",
      "loss: 0.000273  [    0/  780]\n",
      "Test Error: \n",
      "  Avg loss: 0.000563 \n",
      "\n",
      "Epoch 390\n",
      "-------------------------------\n",
      "loss: 0.000175  [    0/  780]\n",
      "Test Error: \n",
      "  Avg loss: 0.000478 \n",
      "\n",
      "Epoch 391\n",
      "-------------------------------\n",
      "loss: 0.000194  [    0/  780]\n",
      "Test Error: \n",
      "  Avg loss: 0.000514 \n",
      "\n",
      "Epoch 392\n",
      "-------------------------------\n",
      "loss: 0.000262  [    0/  780]\n",
      "Test Error: \n",
      "  Avg loss: 0.000389 \n",
      "\n",
      "Epoch 393\n",
      "-------------------------------\n",
      "loss: 0.000228  [    0/  780]\n",
      "Test Error: \n",
      "  Avg loss: 0.000568 \n",
      "\n",
      "Epoch 394\n",
      "-------------------------------\n",
      "loss: 0.000243  [    0/  780]\n",
      "Test Error: \n",
      "  Avg loss: 0.000423 \n",
      "\n",
      "Epoch 395\n",
      "-------------------------------\n",
      "loss: 0.000209  [    0/  780]\n",
      "Test Error: \n",
      "  Avg loss: 0.000510 \n",
      "\n",
      "Epoch 396\n",
      "-------------------------------\n",
      "loss: 0.000190  [    0/  780]\n",
      "Test Error: \n",
      "  Avg loss: 0.000346 \n",
      "\n",
      "Epoch 397\n",
      "-------------------------------\n",
      "loss: 0.000233  [    0/  780]\n",
      "Test Error: \n",
      "  Avg loss: 0.000405 \n",
      "\n",
      "Epoch 398\n",
      "-------------------------------\n",
      "loss: 0.000223  [    0/  780]\n",
      "Test Error: \n",
      "  Avg loss: 0.000343 \n",
      "\n",
      "Epoch 399\n",
      "-------------------------------\n",
      "loss: 0.000130  [    0/  780]\n",
      "Test Error: \n",
      "  Avg loss: 0.000354 \n",
      "\n",
      "Epoch 400\n",
      "-------------------------------\n",
      "loss: 0.000176  [    0/  780]\n",
      "Test Error: \n",
      "  Avg loss: 0.000430 \n",
      "\n",
      "Epoch 401\n",
      "-------------------------------\n",
      "loss: 0.000290  [    0/  780]\n",
      "Test Error: \n",
      "  Avg loss: 0.000378 \n",
      "\n",
      "Epoch 402\n",
      "-------------------------------\n",
      "loss: 0.000304  [    0/  780]\n",
      "Test Error: \n",
      "  Avg loss: 0.000384 \n",
      "\n",
      "Epoch 403\n",
      "-------------------------------\n",
      "loss: 0.000186  [    0/  780]\n",
      "Test Error: \n",
      "  Avg loss: 0.000364 \n",
      "\n",
      "Epoch 404\n",
      "-------------------------------\n",
      "loss: 0.000251  [    0/  780]\n",
      "Test Error: \n",
      "  Avg loss: 0.000392 \n",
      "\n",
      "Epoch 405\n",
      "-------------------------------\n",
      "loss: 0.000155  [    0/  780]\n",
      "Test Error: \n",
      "  Avg loss: 0.000450 \n",
      "\n",
      "Epoch 406\n",
      "-------------------------------\n",
      "loss: 0.000243  [    0/  780]\n",
      "Test Error: \n",
      "  Avg loss: 0.000407 \n",
      "\n",
      "Epoch 407\n",
      "-------------------------------\n",
      "loss: 0.000252  [    0/  780]\n",
      "Test Error: \n",
      "  Avg loss: 0.000431 \n",
      "\n",
      "Epoch 408\n",
      "-------------------------------\n",
      "loss: 0.000181  [    0/  780]\n",
      "Test Error: \n",
      "  Avg loss: 0.000439 \n",
      "\n",
      "Epoch 409\n",
      "-------------------------------\n",
      "loss: 0.000376  [    0/  780]\n",
      "Test Error: \n",
      "  Avg loss: 0.000387 \n",
      "\n",
      "Epoch 410\n",
      "-------------------------------\n",
      "loss: 0.000127  [    0/  780]\n",
      "Test Error: \n",
      "  Avg loss: 0.000402 \n",
      "\n",
      "Epoch 411\n",
      "-------------------------------\n",
      "loss: 0.000168  [    0/  780]\n",
      "Test Error: \n",
      "  Avg loss: 0.000493 \n",
      "\n",
      "Epoch 412\n",
      "-------------------------------\n",
      "loss: 0.000309  [    0/  780]\n",
      "Test Error: \n",
      "  Avg loss: 0.000421 \n",
      "\n",
      "Epoch 413\n",
      "-------------------------------\n",
      "loss: 0.000299  [    0/  780]\n",
      "Test Error: \n",
      "  Avg loss: 0.000418 \n",
      "\n",
      "Epoch 414\n",
      "-------------------------------\n",
      "loss: 0.000237  [    0/  780]\n",
      "Test Error: \n",
      "  Avg loss: 0.000435 \n",
      "\n",
      "Epoch 415\n",
      "-------------------------------\n",
      "loss: 0.000160  [    0/  780]\n",
      "Test Error: \n",
      "  Avg loss: 0.000535 \n",
      "\n",
      "Epoch 416\n",
      "-------------------------------\n",
      "loss: 0.000297  [    0/  780]\n",
      "Test Error: \n",
      "  Avg loss: 0.000371 \n",
      "\n",
      "Epoch 417\n",
      "-------------------------------\n",
      "loss: 0.000261  [    0/  780]\n",
      "Test Error: \n",
      "  Avg loss: 0.000696 \n",
      "\n",
      "Epoch 418\n",
      "-------------------------------\n",
      "loss: 0.000213  [    0/  780]\n",
      "Test Error: \n",
      "  Avg loss: 0.000491 \n",
      "\n",
      "Epoch 419\n",
      "-------------------------------\n",
      "loss: 0.000217  [    0/  780]\n",
      "Test Error: \n",
      "  Avg loss: 0.000327 \n",
      "\n",
      "Epoch 420\n",
      "-------------------------------\n",
      "loss: 0.000120  [    0/  780]\n",
      "Test Error: \n",
      "  Avg loss: 0.000390 \n",
      "\n",
      "Epoch 421\n",
      "-------------------------------\n",
      "loss: 0.000266  [    0/  780]\n",
      "Test Error: \n",
      "  Avg loss: 0.000339 \n",
      "\n",
      "Epoch 422\n",
      "-------------------------------\n",
      "loss: 0.000159  [    0/  780]\n",
      "Test Error: \n",
      "  Avg loss: 0.000382 \n",
      "\n",
      "Epoch 423\n",
      "-------------------------------\n",
      "loss: 0.000187  [    0/  780]\n",
      "Test Error: \n",
      "  Avg loss: 0.000346 \n",
      "\n",
      "Epoch 424\n",
      "-------------------------------\n",
      "loss: 0.000164  [    0/  780]\n",
      "Test Error: \n",
      "  Avg loss: 0.000442 \n",
      "\n",
      "Epoch 425\n",
      "-------------------------------\n",
      "loss: 0.000145  [    0/  780]\n",
      "Test Error: \n",
      "  Avg loss: 0.000367 \n",
      "\n",
      "Epoch 426\n",
      "-------------------------------\n",
      "loss: 0.000221  [    0/  780]\n",
      "Test Error: \n",
      "  Avg loss: 0.000453 \n",
      "\n",
      "Epoch 427\n",
      "-------------------------------\n",
      "loss: 0.000131  [    0/  780]\n",
      "Test Error: \n",
      "  Avg loss: 0.000365 \n",
      "\n",
      "Epoch 428\n",
      "-------------------------------\n",
      "loss: 0.000164  [    0/  780]\n",
      "Test Error: \n",
      "  Avg loss: 0.000405 \n",
      "\n",
      "Epoch 429\n",
      "-------------------------------\n",
      "loss: 0.000185  [    0/  780]\n",
      "Test Error: \n",
      "  Avg loss: 0.000438 \n",
      "\n",
      "Epoch 430\n",
      "-------------------------------\n",
      "loss: 0.000222  [    0/  780]\n",
      "Test Error: \n",
      "  Avg loss: 0.000403 \n",
      "\n",
      "Epoch 431\n",
      "-------------------------------\n",
      "loss: 0.000217  [    0/  780]\n",
      "Test Error: \n",
      "  Avg loss: 0.000460 \n",
      "\n",
      "Epoch 432\n",
      "-------------------------------\n",
      "loss: 0.000137  [    0/  780]\n",
      "Test Error: \n",
      "  Avg loss: 0.000330 \n",
      "\n",
      "Epoch 433\n",
      "-------------------------------\n",
      "loss: 0.000204  [    0/  780]\n",
      "Test Error: \n",
      "  Avg loss: 0.000480 \n",
      "\n",
      "Epoch 434\n",
      "-------------------------------\n",
      "loss: 0.000308  [    0/  780]\n",
      "Test Error: \n",
      "  Avg loss: 0.000373 \n",
      "\n",
      "Epoch 435\n",
      "-------------------------------\n",
      "loss: 0.000275  [    0/  780]\n",
      "Test Error: \n",
      "  Avg loss: 0.000523 \n",
      "\n",
      "Epoch 436\n",
      "-------------------------------\n",
      "loss: 0.000221  [    0/  780]\n",
      "Test Error: \n",
      "  Avg loss: 0.000477 \n",
      "\n",
      "Epoch 437\n",
      "-------------------------------\n",
      "loss: 0.000183  [    0/  780]\n",
      "Test Error: \n",
      "  Avg loss: 0.000358 \n",
      "\n",
      "Epoch 438\n",
      "-------------------------------\n",
      "loss: 0.000264  [    0/  780]\n",
      "Test Error: \n",
      "  Avg loss: 0.000348 \n",
      "\n",
      "Epoch 439\n",
      "-------------------------------\n",
      "loss: 0.000128  [    0/  780]\n",
      "Test Error: \n",
      "  Avg loss: 0.000374 \n",
      "\n",
      "Epoch 440\n",
      "-------------------------------\n",
      "loss: 0.000173  [    0/  780]\n",
      "Test Error: \n",
      "  Avg loss: 0.000454 \n",
      "\n",
      "Epoch 441\n",
      "-------------------------------\n",
      "loss: 0.000376  [    0/  780]\n",
      "Test Error: \n",
      "  Avg loss: 0.000399 \n",
      "\n",
      "Epoch 442\n",
      "-------------------------------\n",
      "loss: 0.000271  [    0/  780]\n",
      "Test Error: \n",
      "  Avg loss: 0.000384 \n",
      "\n",
      "Epoch 443\n",
      "-------------------------------\n",
      "loss: 0.000114  [    0/  780]\n",
      "Test Error: \n",
      "  Avg loss: 0.000388 \n",
      "\n",
      "Epoch 444\n",
      "-------------------------------\n",
      "loss: 0.000220  [    0/  780]\n",
      "Test Error: \n",
      "  Avg loss: 0.000617 \n",
      "\n",
      "Epoch 445\n",
      "-------------------------------\n",
      "loss: 0.000275  [    0/  780]\n",
      "Test Error: \n",
      "  Avg loss: 0.000381 \n",
      "\n",
      "Epoch 446\n",
      "-------------------------------\n",
      "loss: 0.000275  [    0/  780]\n",
      "Test Error: \n",
      "  Avg loss: 0.000342 \n",
      "\n",
      "Epoch 447\n",
      "-------------------------------\n",
      "loss: 0.000113  [    0/  780]\n",
      "Test Error: \n",
      "  Avg loss: 0.000308 \n",
      "\n",
      "Epoch 448\n",
      "-------------------------------\n",
      "loss: 0.000115  [    0/  780]\n",
      "Test Error: \n",
      "  Avg loss: 0.000418 \n",
      "\n",
      "Epoch 449\n",
      "-------------------------------\n",
      "loss: 0.000251  [    0/  780]\n",
      "Test Error: \n",
      "  Avg loss: 0.000512 \n",
      "\n",
      "Epoch 450\n",
      "-------------------------------\n",
      "loss: 0.000234  [    0/  780]\n",
      "Test Error: \n",
      "  Avg loss: 0.000521 \n",
      "\n",
      "Epoch 451\n",
      "-------------------------------\n",
      "loss: 0.000175  [    0/  780]\n",
      "Test Error: \n",
      "  Avg loss: 0.000472 \n",
      "\n",
      "Epoch 452\n",
      "-------------------------------\n",
      "loss: 0.000422  [    0/  780]\n",
      "Test Error: \n",
      "  Avg loss: 0.000491 \n",
      "\n",
      "Epoch 453\n",
      "-------------------------------\n",
      "loss: 0.000172  [    0/  780]\n",
      "Test Error: \n",
      "  Avg loss: 0.000399 \n",
      "\n",
      "Epoch 454\n",
      "-------------------------------\n",
      "loss: 0.000149  [    0/  780]\n",
      "Test Error: \n",
      "  Avg loss: 0.000383 \n",
      "\n",
      "Epoch 455\n",
      "-------------------------------\n",
      "loss: 0.000171  [    0/  780]\n",
      "Test Error: \n",
      "  Avg loss: 0.000401 \n",
      "\n",
      "Epoch 456\n",
      "-------------------------------\n",
      "loss: 0.000178  [    0/  780]\n",
      "Test Error: \n",
      "  Avg loss: 0.000420 \n",
      "\n",
      "Epoch 457\n",
      "-------------------------------\n",
      "loss: 0.000171  [    0/  780]\n",
      "Test Error: \n",
      "  Avg loss: 0.000388 \n",
      "\n",
      "Epoch 458\n",
      "-------------------------------\n",
      "loss: 0.000277  [    0/  780]\n",
      "Test Error: \n",
      "  Avg loss: 0.000376 \n",
      "\n",
      "Epoch 459\n",
      "-------------------------------\n",
      "loss: 0.000154  [    0/  780]\n",
      "Test Error: \n",
      "  Avg loss: 0.000298 \n",
      "\n",
      "Epoch 460\n",
      "-------------------------------\n",
      "loss: 0.000121  [    0/  780]\n",
      "Test Error: \n",
      "  Avg loss: 0.000417 \n",
      "\n",
      "Epoch 461\n",
      "-------------------------------\n",
      "loss: 0.000265  [    0/  780]\n",
      "Test Error: \n",
      "  Avg loss: 0.000351 \n",
      "\n",
      "Epoch 462\n",
      "-------------------------------\n",
      "loss: 0.000156  [    0/  780]\n",
      "Test Error: \n",
      "  Avg loss: 0.000451 \n",
      "\n",
      "Epoch 463\n",
      "-------------------------------\n",
      "loss: 0.000245  [    0/  780]\n",
      "Test Error: \n",
      "  Avg loss: 0.000558 \n",
      "\n",
      "Epoch 464\n",
      "-------------------------------\n",
      "loss: 0.000207  [    0/  780]\n",
      "Test Error: \n",
      "  Avg loss: 0.000456 \n",
      "\n",
      "Epoch 465\n",
      "-------------------------------\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loss: 0.000282  [    0/  780]\n",
      "Test Error: \n",
      "  Avg loss: 0.000534 \n",
      "\n",
      "Epoch 466\n",
      "-------------------------------\n",
      "loss: 0.000295  [    0/  780]\n",
      "Test Error: \n",
      "  Avg loss: 0.000403 \n",
      "\n",
      "Epoch 467\n",
      "-------------------------------\n",
      "loss: 0.000193  [    0/  780]\n",
      "Test Error: \n",
      "  Avg loss: 0.000436 \n",
      "\n",
      "Epoch 468\n",
      "-------------------------------\n",
      "loss: 0.000140  [    0/  780]\n",
      "Test Error: \n",
      "  Avg loss: 0.000344 \n",
      "\n",
      "Epoch 469\n",
      "-------------------------------\n",
      "loss: 0.000141  [    0/  780]\n",
      "Test Error: \n",
      "  Avg loss: 0.000415 \n",
      "\n",
      "Epoch 470\n",
      "-------------------------------\n",
      "loss: 0.000165  [    0/  780]\n",
      "Test Error: \n",
      "  Avg loss: 0.000449 \n",
      "\n",
      "Epoch 471\n",
      "-------------------------------\n",
      "loss: 0.000198  [    0/  780]\n",
      "Test Error: \n",
      "  Avg loss: 0.000415 \n",
      "\n",
      "Epoch 472\n",
      "-------------------------------\n",
      "loss: 0.000196  [    0/  780]\n",
      "Test Error: \n",
      "  Avg loss: 0.000335 \n",
      "\n",
      "Epoch 473\n",
      "-------------------------------\n",
      "loss: 0.000173  [    0/  780]\n",
      "Test Error: \n",
      "  Avg loss: 0.000383 \n",
      "\n",
      "Epoch 474\n",
      "-------------------------------\n",
      "loss: 0.000201  [    0/  780]\n",
      "Test Error: \n",
      "  Avg loss: 0.000395 \n",
      "\n",
      "Epoch 475\n",
      "-------------------------------\n",
      "loss: 0.000168  [    0/  780]\n",
      "Test Error: \n",
      "  Avg loss: 0.000498 \n",
      "\n",
      "Epoch 476\n",
      "-------------------------------\n",
      "loss: 0.000280  [    0/  780]\n",
      "Test Error: \n",
      "  Avg loss: 0.000425 \n",
      "\n",
      "Epoch 477\n",
      "-------------------------------\n",
      "loss: 0.000175  [    0/  780]\n",
      "Test Error: \n",
      "  Avg loss: 0.000397 \n",
      "\n",
      "Epoch 478\n",
      "-------------------------------\n",
      "loss: 0.000185  [    0/  780]\n",
      "Test Error: \n",
      "  Avg loss: 0.000337 \n",
      "\n",
      "Epoch 479\n",
      "-------------------------------\n",
      "loss: 0.000135  [    0/  780]\n",
      "Test Error: \n",
      "  Avg loss: 0.000462 \n",
      "\n",
      "Epoch 480\n",
      "-------------------------------\n",
      "loss: 0.000195  [    0/  780]\n",
      "Test Error: \n",
      "  Avg loss: 0.000402 \n",
      "\n",
      "Epoch 481\n",
      "-------------------------------\n",
      "loss: 0.000254  [    0/  780]\n",
      "Test Error: \n",
      "  Avg loss: 0.000441 \n",
      "\n",
      "Epoch 482\n",
      "-------------------------------\n",
      "loss: 0.000180  [    0/  780]\n",
      "Test Error: \n",
      "  Avg loss: 0.000343 \n",
      "\n",
      "Epoch 483\n",
      "-------------------------------\n",
      "loss: 0.000196  [    0/  780]\n",
      "Test Error: \n",
      "  Avg loss: 0.000436 \n",
      "\n",
      "Epoch 484\n",
      "-------------------------------\n",
      "loss: 0.000190  [    0/  780]\n",
      "Test Error: \n",
      "  Avg loss: 0.000322 \n",
      "\n",
      "Epoch 485\n",
      "-------------------------------\n",
      "loss: 0.000236  [    0/  780]\n",
      "Test Error: \n",
      "  Avg loss: 0.000420 \n",
      "\n",
      "Epoch 486\n",
      "-------------------------------\n",
      "loss: 0.000289  [    0/  780]\n",
      "Test Error: \n",
      "  Avg loss: 0.000369 \n",
      "\n",
      "Epoch 487\n",
      "-------------------------------\n",
      "loss: 0.000179  [    0/  780]\n",
      "Test Error: \n",
      "  Avg loss: 0.000372 \n",
      "\n",
      "Epoch 488\n",
      "-------------------------------\n",
      "loss: 0.000176  [    0/  780]\n",
      "Test Error: \n",
      "  Avg loss: 0.000460 \n",
      "\n",
      "Epoch 489\n",
      "-------------------------------\n",
      "loss: 0.000225  [    0/  780]\n",
      "Test Error: \n",
      "  Avg loss: 0.000639 \n",
      "\n",
      "Epoch 490\n",
      "-------------------------------\n",
      "loss: 0.000513  [    0/  780]\n",
      "Test Error: \n",
      "  Avg loss: 0.000504 \n",
      "\n",
      "Epoch 491\n",
      "-------------------------------\n",
      "loss: 0.000150  [    0/  780]\n",
      "Test Error: \n",
      "  Avg loss: 0.000433 \n",
      "\n",
      "Epoch 492\n",
      "-------------------------------\n",
      "loss: 0.000326  [    0/  780]\n",
      "Test Error: \n",
      "  Avg loss: 0.000436 \n",
      "\n",
      "Epoch 493\n",
      "-------------------------------\n",
      "loss: 0.000244  [    0/  780]\n",
      "Test Error: \n",
      "  Avg loss: 0.000384 \n",
      "\n",
      "Epoch 494\n",
      "-------------------------------\n",
      "loss: 0.000230  [    0/  780]\n",
      "Test Error: \n",
      "  Avg loss: 0.000375 \n",
      "\n",
      "Epoch 495\n",
      "-------------------------------\n",
      "loss: 0.000218  [    0/  780]\n",
      "Test Error: \n",
      "  Avg loss: 0.000371 \n",
      "\n",
      "Epoch 496\n",
      "-------------------------------\n",
      "loss: 0.000182  [    0/  780]\n",
      "Test Error: \n",
      "  Avg loss: 0.000408 \n",
      "\n",
      "Epoch 497\n",
      "-------------------------------\n",
      "loss: 0.000148  [    0/  780]\n",
      "Test Error: \n",
      "  Avg loss: 0.000392 \n",
      "\n",
      "Epoch 498\n",
      "-------------------------------\n",
      "loss: 0.000227  [    0/  780]\n",
      "Test Error: \n",
      "  Avg loss: 0.000551 \n",
      "\n",
      "Epoch 499\n",
      "-------------------------------\n",
      "loss: 0.000167  [    0/  780]\n",
      "Test Error: \n",
      "  Avg loss: 0.000453 \n",
      "\n",
      "Epoch 500\n",
      "-------------------------------\n",
      "loss: 0.000139  [    0/  780]\n",
      "Test Error: \n",
      "  Avg loss: 0.000351 \n",
      "\n",
      "Done!\n"
     ]
    }
   ],
   "source": [
    "epochs = 500\n",
    "for t in range(epochs):\n",
    "    print(f\"Epoch {t+1}\\n-------------------------------\")\n",
    "    train(train_dataloader, model, loss_fn, optimizer)\n",
    "    validate(validation_dataloader, model, loss_fn)\n",
    "print(\"Done!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c33cdf0a",
   "metadata": {},
   "source": [
    "Save the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "id": "179c9713",
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.save(model, 'model_cnn1_cie.pth')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "455b173e",
   "metadata": {},
   "source": [
    "load the model and apply it on the test dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "79108af7",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = torch.load('model_cnn1_cie.pth')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "eaa9d4f3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(11.4029, device='cuda:0')\n"
     ]
    }
   ],
   "source": [
    "with torch.no_grad():\n",
    "    \n",
    "    loss = 0\n",
    "    for X, y in test_dataloader:\n",
    "        X = X.float()\n",
    "        y = y.float()\n",
    "        X, y = X.to(device), y.to(device)\n",
    "        pred = model(X)\n",
    "        pred_cpu = pred.cpu()\n",
    "        pred_real = scaler.inverse_transform(pred_cpu)\n",
    "        pred_real = torch.Tensor(pred_real)\n",
    "        pred_gpu = pred_real.to(device)\n",
    "        loss += loss_fn(pred_gpu, y)\n",
    "        \n",
    "    print(loss/98)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "73df122b",
   "metadata": {},
   "source": [
    "Feature Map visualization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "be8f2fdf",
   "metadata": {},
   "outputs": [],
   "source": [
    "model_weights =[]\n",
    "conv_layers = []\n",
    "pool_layers = []\n",
    "relu_layers = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "b5db3863",
   "metadata": {},
   "outputs": [],
   "source": [
    "model_children = list(model.children())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "dd7a3b12",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Conv2d(1, 3, kernel_size=(50, 2), stride=(1, 1)), MaxPool2d(kernel_size=(3, 1), stride=(3, 1), padding=0, dilation=1, ceil_mode=False), Conv2d(3, 6, kernel_size=(10, 2), stride=(1, 1)), MaxPool2d(kernel_size=(2, 1), stride=(2, 1), padding=0, dilation=1, ceil_mode=False), ReLU(), Linear(in_features=48, out_features=16, bias=True), Linear(in_features=16, out_features=2, bias=True)]\n"
     ]
    }
   ],
   "source": [
    "print(model_children)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "1d23934a",
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in range(len(model_children)):\n",
    "    if type(model_children[i]) == nn.Conv2d:\n",
    "        model_weights.append(model_children[i].weight)\n",
    "        conv_layers.append(model_children[i])\n",
    "    if type(model_children[i]) == nn.MaxPool2d:\n",
    "        pool_layers.append(model_children[i])\n",
    "    if type(model_children[i]) == nn.ReLU:\n",
    "        relu_layers.append(model_children[i])  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "4ddfcb4f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Conv2d(1, 3, kernel_size=(50, 2), stride=(1, 1)), Conv2d(3, 6, kernel_size=(10, 2), stride=(1, 1))]\n"
     ]
    }
   ],
   "source": [
    "print(conv_layers)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "ca0f379d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(100, 4)\n"
     ]
    }
   ],
   "source": [
    "test_input = x_test[0]\n",
    "print(test_input.shape)\n",
    "test_input = torch.Tensor(test_input)\n",
    "test_input = test_input.unsqueeze(0)\n",
    "test_input = test_input.to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "a7544d96",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[MaxPool2d(kernel_size=(3, 1), stride=(3, 1), padding=0, dilation=1, ceil_mode=False), MaxPool2d(kernel_size=(2, 1), stride=(2, 1), padding=0, dilation=1, ceil_mode=False)]\n"
     ]
    }
   ],
   "source": [
    "print(pool_layers)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "58f8602f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[ReLU()]\n"
     ]
    }
   ],
   "source": [
    "print(relu_layers)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "6ef54a3f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([3, 51, 3])\n",
      "torch.Size([3, 17, 3])\n",
      "torch.Size([3, 17, 3])\n",
      "torch.Size([6, 8, 2])\n",
      "torch.Size([6, 4, 2])\n",
      "torch.Size([6, 4, 2])\n"
     ]
    }
   ],
   "source": [
    "outputs = []\n",
    "\n",
    "conv_layer1 = conv_layers[0]\n",
    "feature_per_layer = conv_layer1(test_input)\n",
    "print(feature_per_layer.shape)\n",
    "\n",
    "pool_layer1 = pool_layers[0]\n",
    "feature_per_layer1 = pool_layer1(feature_per_layer)\n",
    "print(feature_per_layer1.shape)\n",
    "\n",
    "relu_layer1 = relu_layers[0]\n",
    "feature_per_layer2 = relu_layer1(feature_per_layer1)\n",
    "print(feature_per_layer2.shape)\n",
    "\n",
    "conv_layer2 = conv_layers[1]\n",
    "feature_per_layer3 = conv_layer2(feature_per_layer2)\n",
    "print(feature_per_layer3.shape)\n",
    "\n",
    "pool_layer2 = pool_layers[1]\n",
    "feature_per_layer4 = pool_layer2(feature_per_layer3)\n",
    "print(feature_per_layer4.shape)\n",
    "\n",
    "relu_layer2 = relu_layers[0]\n",
    "feature_per_layer5 = relu_layer1(feature_per_layer4)\n",
    "print(feature_per_layer5.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "def254c0",
   "metadata": {},
   "outputs": [],
   "source": [
    "gray_scale = torch.sum(feature_per_layer,0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "eb32a866",
   "metadata": {},
   "outputs": [],
   "source": [
    "gray_scale = gray_scale / feature_per_layer.shape[0]\n",
    "gray_scale = gray_scale.data.cpu().numpy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "e6bfb04a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(51, 3)\n"
     ]
    }
   ],
   "source": [
    "print(gray_scale.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "81ac8899",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<matplotlib.image.AxesImage at 0x1f85d1f0730>"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAMIAAAq9CAYAAABF5f1rAAAAOXRFWHRTb2Z0d2FyZQBNYXRwbG90bGliIHZlcnNpb24zLjUuMSwgaHR0cHM6Ly9tYXRwbG90bGliLm9yZy/YYfK9AAAACXBIWXMAAAsTAAALEwEAmpwYAAAnrElEQVR4nO3dX4ju+V3Y8ddn5jlnN9kk5k+TdEliI2URRVBhCUKgUNOU1JYmNxaFykIDe1EFpYWS9q434pX0pjdLlW6p1QZUEkRqw1aRgtisVlvTjU0QjUu22ZqYvya7e858e7EjbHWPO3vO/H6PZ/J+wfLM88xMPp8nM+/znT/wm1lrSb7enRx7geQvg0JIFEKCQkhQCAkKIQGHPYedvvq+dXjja/ccubl7P/XssVfYxLNvvPfYK1y6r/2fJ/9orfXGF3vdriEc3vhab/3Rf7znyM098IN/cOwVNvEH/+hbjr3Cpfv4j/6TW36w+tIoUQgJCiFBISQohASFkKAQEhRCgkJIUAgJCiFBISQohASFkKAQEhRCgkJIUAgJCiFBISQohAR3GMLMvGdmfndmPjkzH7ispZK93XYIM3OKf42/g2/F98/Mt17WYsme7uREeAc+udb6vbXWs/gZvPdy1kr2dSchvAV/+IL7T54/9v+ZmYdn5vGZefzsS1+5g3HJdu4khHmRx/7cn99Zaz2y1npwrfXgyavvu4NxyXbuJIQn8bYX3H8rPn1n6yTHcSchfBQPzMw3zcx1fB8+fDlrJfu67YsAr7VuzMwP4Zdwip9ca33s0jZLdnRHV8Nea/0ifvGSdkmOpt8sJwohQSEkKIQEhZCgEBIUQoJCSFAICQohQSEkKIQEhZCgEBIUQoJCSFAICQohQSEkKIQEhZDgDq9icTv+3KXw7nLrq1899grbeLHrGF5hnQiJQkhQCAkKIUEhJCiEBIWQoBASFEKCQkhQCAkKIUEhJCiEBIWQoBASFEKCQkhQCAkKIUEhJCiEBIWQoBASFEKCQkhQCAkKIUEhJCiEBIWQoBASFEKCQkhQCAkKIUEhJCiEBIWQoBASFEKCQkhQCAkKIUEhJCiEBIWQoBASFEKCQkhQCAkKIQGHvQeus9l75KbOnnnm2CtsYx17gX11IiQKIUEhJCiEBIWQoBASFEKCQkhQCAkKIUEhJCiEBIWQoBASFEKCQkhQCAkKIUEhJCiEBIWQoBASFEKCQkhQCAkKIUEhJCiEBIWQoBASFEKCQkhQCAkKIUEhJCiEBIWQoBASFEKCQkhQCAkKIUEhJCiEBIWQoBASFEKCQkhQCAkKIUEhJCiEBIWQgMPuE9fuE7e1rtoTet5czad1S50IiUJIUAgJCiFBISQohASFkKAQEhRCgkJIUAgJCiFBISQohASFkKAQEhRCgkJIUAgJCiFBISQohASFkKAQEhRCgkJIUAgJCiFBISQohASFkKAQEhRCgkJIUAgJCiFBISQohASFkKAQEhRCgkJIUAgJCiFBISQohASFkKAQEhRCgkJIUAgJCiEBh12nrbHOZteRmzs5PfYGm5gbx95gX50IiUJIUAgJCiFBISQohASFkKAQEhRCgkJIUAgJCiFBISQohASFkKAQEhRCgkJIUAgJCiFBISQohASFkKAQEhRCgkJIUAgJCiFBISQohASFkKAQEhRCgkJIUAgJCiFBISQohASFkKAQEhRCgkJIUAgJCiFBISQohASFkKAQEhRCgkJIUAgJCiEBh70HrrPZe+SmTu575bFX2MTJzWNvsK9OhEQhJCiEBIWQoBASFEKCQkhQCAkKIUEhJCiEBIWQoBASFEKCQkhQCAkKIUEhJCiEBIWQoBASFEKCQkhQCAkKIUEhJCiEBIWQoBASFEKCQkhQCAkKIUEhJCiEBIWQoBASFEKCQkhQCAkKIUEhJCiEBIWQoBASFEKCQkhQCAkKIUEhJCiEBIWQoBAScDj2Ane7k294zbFX2MTcOPYG++pESBRCgkJIUAgJCiFBISQohASFkKAQEhRCgkJIUAgJCiFBISQohASFkKAQEhRCgkJIUAgJCiFBISQohASFkKAQEhRCgkJIUAgJCiFBISQohASFkKAQEhRCgkJIUAgJCiFBISQohASFkKAQEhRCgkJIUAgJCiFBISQohASFkKAQEhRCgkJIUAgJOOw9cE7W3iM3dfaG1xx7hU3M1fowvaROhEQhJCiEBIWQoBASFEKCQkhQCAkKIUEhJCiEBIWQoBASFEKCQkhQCAkKIUEhJCiEBIWQoBASFEKCQkhQCAkKIUEhJCiEBIWQoBASFEKCQkhQCAkKIUEhJCiEBIWQoBASFEKCQkhQCAkKIUEhJCiEBIWQoBASFEKCQkhQCAkKIUEhJCiEBIWQgMOu02Y5OaxdR27t2Te88tgrbOLs9Ngb7KsTIVEICQohQSEkKIQEhZCgEBIUQoJCSFAICQohQSEkKIQEhZCgEBIUQoJCSFAICQohQSEkKIQEhZCgEBIUQoJCSFAICQohQSEkKIQEhZCgEBIUQoJCSFAICQohQSEkKIQEhZCgEBIUQoJCSFAICQohQSEkKIQEhZCgEBIUQoJCSFAICQohQSEkKIQEHPYcNsPJ6c09R27ua2+4fuwVNnF27dgb7KsTIVEICQohQSEkKIQEhZCgEBIUQoJCSFAICQohQSEkKIQEhZCgEBIUQoJCSFAICQohQSEkKIQEhZCgEBIUQoJCSFAICQohQSEkKIQEhZCgEBIUQoJCSFAICQohQSEkKIQEhZCgEBIUQoJCSFAICQohQSEkKIQEhZCgEBIUQoJCSFAICQohQSEk4LDnsJnl2rWbe47c3DOvvZr/lpxdO/YG+7qaH8XkZSqERCEkKIQEhZCgEBIUQoJCSFAICQohQSEkKIQEhZCgEBIUQoJCSFAICQohQSEkKIQEhZCgEBJcIISZ+cmZeXpmfucFj71+Zj4yM584v33dtmsm27rIifBv8Z4/89gH8Nha6wE8dn4/uWu9ZAhrrV/F5/7Mw+/Fo+cvP4r3Xe5ayb5u93uEN6+1noLz2zfd6g1n5uGZeXxmHr/xxT+5zXHJtjb/Znmt9cha68G11oOH17xy63HJbbndED4zM/fD+e3Tl7dSsr/bDeHDeOj85YfwoctZJzmOi/z49Kfxa/jmmXlyZt6PH8O7Z+YTePf5/eSu9ZKXhV9rff8tXvWuS94lOZp+s5wohASFkKAQEhRCgkJIUAgJCiFBISQohASFkKAQEhRCgkJIUAgJCiFBISQohASFkKAQEhRCggtcxeIyzXD9cHPPkZt79jVz7BU2cXb92BvsqxMhUQgJCiFBISQohASFkKAQEhRCgkJIUAgJCiFBISQohASFkKAQEhRCgkJIUAgJCiFBISQohASFkKAQEhRCgkJIUAgJCiFBISQohASFkKAQEhRCgkJIUAgJCiFBISQohASFkKAQEhRCgkJIUAgJCiFBISQohASFkKAQEhRCgkJIUAgJCiFBISTgsOewmeXe68/tOXJzX3jlsTfYxs171rFX2FUnQqIQEhRCgkJIUAgJCiFBISQohASFkKAQEhRCgkJIUAgJCiFBISQohASFkKAQEhRCgkJIUAgJCiFBISQohASFkKAQEhRCgkJIUAgJCiFBISQohASFkKAQEhRCgkJIUAgJCiFBISQohASFkKAQEhRCgkJIUAgJCiFBISQohASFkKAQEhRCgkJIUAgJOOw57GSWew839hy5uc++ch17hU2cXb+az+tWOhEShZCgEBIUQoJCSFAICQohQSEkKIQEhZCgEBIUQoJCSFAICQohQSEkKIQEhZCgEBIUQoJCSFAICQohQSEkKIQEhZCgEBIUQoJCSFAICQohQSEkKIQEhZCgEBIUQoJCSFAICQohQSEkKIQEhZCgEBIUQoJCSFAICQohQSEkKIQEhZCgEBIUQgIOew4byz2nN/Ycubmb96xjr7CJdUWf1610IiQKIUEhJCiEBIWQoBASFEKCQkhQCAkKIUEhJCiEBIWQoBASFEKCQkhQCAkKIUEhJCiEBIWQoBASFEKCQkhQCAkKIUEhJCiEBIWQoBASFEKCQkhQCAkKIUEhJCiEBIWQoBASFEKCQkhQCAkKIUEhJCiEBIWQoBASFEKCQkhQCAkKIUEhJCiEBBz2HHYyy/XTm3uO3Ny6vo69wjbuu3HsDXbViZAohASFkKAQEhRCgkJIUAgJCiFBISQohASFkKAQEhRCgkJIUAgJCiFBISQohASFkKAQEhRCgkJIUAgJCiFBISQohASFkKAQEhRCgkJIUAgJCiFBISQohASFkKAQEhRCgkJIUAgJCiFBISQohASFkKAQEhRCgkJIUAgJCiFBISQohASFkKAQEhRCAg67D5ybe4/c1DqsY6+wiVe++mvHXmFXnQiJQkhQCAkKIUEhJCiEBIWQoBASFEKCQkhQCAkKIUEhJCiEBIWQoBASFEKCQkhQCAkKIUEhJCiEBIWQoBASFEKCQkhQCAkKIUEhJCiEBIWQoBASFEKCQkhQCAkKIUEhJCiEBIWQoBASFEKCQkhQCAkKIUEhJCiEBIWQoBASFEKCQkhQCAkKIQGHYy9w17t2duwNNvGm13z52Ctcuo//Ba/rREgUQoJCSFAICQohQSEkKIQEhZCgEBIUQoJCSFAICQohQSEkKIQEhZCgEBIUQoJCSFAICQohQSEkKIQEhZCgEBIUQoJCSFAICQohQSEkKIQEhZCgEBIUQoJCSFAICQohQSEkKIQEhZCgEBIUQoJCSFAICQohQSEkKIQEhZCgEBIUQoJCSFAICTjsPfDGOt175KbmdB17hU287VV/fOwVdtWJkCiEBIWQoBASFEKCQkhQCAkKIUEhJCiEBIWQoBASFEKCQkhQCAkKIUEhJCiEBIWQoBASFEKCQkhQCAkKIUEhJCiEBIWQoBASFEKCQkhQCAkKIUEhJCiEBIWQoBASFEKCQkhQCAkKIUEhJCiEBIWQoBASFEKCQkhQCAkKIUEhJCiEBIWQoBAScNhz2Nkaz9zYdeT2TtaxN9jEN77ij4+9wq46ERKFkKAQEhRCgkJIUAgJCiFBISQohASFkKAQEhRCgkJIUAgJCiFBISQohASFkKAQEhRCgkJIUAgJCiFBISQohASFkKAQEhRCgkJIUAgJCiFBISQohASFkKAQEhRCgkJIUAgJCiFBISQohASFkKAQEhRCgkJIUAgJCiFBISQohASFkKAQEhRCgkJIwGHPYWdrfOW563uO3NzMOvYKm/jGez577BV21YmQKIQEhZCgEBIUQoJCSFAICQohQSEkKIQEhZCgEBIUQoJCSFAICQohQSEkKIQEhZCgEBIUQoJCSHCBEGbmbTPzyzPzxMx8bGZ++Pzx18/MR2bmE+e3r9t+3WQbFzkRbuCfrrW+Bd+FH5yZb8UH8Nha6wE8dn4/uSu9ZAhrrafWWr95/vKX8ATegvfi0fM3exTv22jHZHMv63uEmXk7vhO/jjevtZ7i+Vjwplu8z8Mz8/jMPP7cF756h+sm27hwCDPzKvwsfmSt9cWLvt9a65G11oNrrQevfcMrbmfHZHMXCmFmrnk+gp9aa/3c+cOfmZn7z19/P57eZsVkexf5qdHgJ/DEWuvHX/CqD+Oh85cfwocuf71kHxe5GvY78QP4nzPzW+eP/Qv8GD44M+/Hp/C9m2yY7OAlQ1hr/VfMLV79rstdJzmOfrOcKIQEhZCgEBIUQoJCSFAICQohQSEkKIQEhZCgEBIUQoJCSFAICQohQSEkKIQEhZCgEBIUQoKLXc7l0pyt8SfPXttz5PZudX2Pu9xfPXz+2CvsqhMhUQgJCiFBISQohASFkKAQEhRCgkJIUAgJCiFBISQohASFkKAQEhRCgkJIUAgJCiFBISQohASFkKAQEhRCgkJIUAgJCiFBISQohASFkKAQEhRCgkJIUAgJCiFBISQohASFkKAQEhRCgkJIUAgJCiFBISQohASFkKAQEhRCgkJIUAgJCiFBISTgsOewszW+9uy1PUfmNr3h9MvHXmFXnQiJQkhQCAkKIUEhJCiEBIWQoBASFEKCQkhQCAkKIUEhJCiEBIWQoBASFEKCQkhQCAkKIUEhJCiEBIWQoBASFEKCQkhQCAkKIUEhJCiEBIWQoBASFEKCQkhQCAkKIUEhJCiEBIWQoBASFEKCQkhQCAkKIUEhJCiEBIWQoBASFEKCQkhQCAkKIUEhJOCw57B1Np57dteR21vHXmAbrz/52rFX2FUnQqIQEhRCgkJIUAgJCiFBISQohASFkKAQEhRCgkJIUAgJCiFBISQohASFkKAQEhRCgkJIUAgJCiFBISQohASFkKAQEhRCgkJIUAgJCiFBISQohASFkKAQEhRCgkJIUAgJCiFBISQohASFkKAQEhRCgkJIUAgJCiFBISQohASFkKAQEhRCgkJIwGHfcePmjdq7G7z25OzYK+yqz8pEISQohASFkKAQEhRCgkJIUAgJCiFBISQohASFkKAQEhRCgkJIUAgJCiFBISQohASFkKAQEhRCgkJIUAgJCiFBISQohASFkKAQEhRCgkJIUAgJCiFBISQohASFkKAQEhRCgkJIUAgJCiFBISQohASFkKAQEhRCgkJIUAgJCiFBISQohASFkIDDrtMW68bVam9Oz469wiZefbLvp8axXa3PyuQ2FUKiEBIUQoJCSFAICQohQSEkKIQEhZCgEBIUQoJCSFAICQohQSEkKIQEhZCgEBIUQoJCSFAICQohQSEkKIQEhZCgEBIUQoJCSFAICQohQSEkKIQEhZCgEBIUQoJCSFAICQohQSEkKIQEhZCgEBIUQoJCSFAICQohQSEkKIQEhZCgEBIUQoJCSMBh12kLz82uIzd3euwFtvGKuX7sFXbViZAohASFkKAQEhRCgkJIUAgJCiFBISQohASFkKAQEhRCgkJIUAgJCiFBISQohASFkKAQEhRCgkJIUAgJCiFBISQohASFkKAQEhRCgkJIUAgJCiFBISQohASFkKAQEhRCgkJIUAgJCiFBISQohASFkKAQEhRCgkJIUAgJCiFBISQohASFkKAQEnDYddoac+OqtXfz2Ats4nSu2sfpL/b19WyTWyiERCEkKIQEhZCgEBIUQoJCSFAICQohQSEkKIQEhZCgEBIUQoJCSFAICQohQSEkKIQEhZCgEBIUQoJCSFAICQohQSEkKIQEhZCgEBIUQoJCSFAICQohQSEkKIQEhZCgEBIUQoJCSFAICQohQSEkKIQEhZCgEBIUQoJCSFAICQohQSEkKIQEHHadtpjnZteRmztZx94gl6ATIVEICQohQSEkKIQEhZCgEBIUQoJCSFAICQohQSEkKIQEhZCgEBIUQoJCSFAICQohQSEkKIQEhZCgEBIUQoJCSFAICQohQSEkKIQEhZCgEBIUQoJCSFAICQohQSEkKIQEhZCgEBIUQoJCSFAICQohQSEkKIQEhZCgEBIUQoJCSFAICQohQSEk4LDrtMXJs7PryK2tK/pPyTPruWOvsKsr+mFMXp5CSBRCgkJIUAgJCiFBISQohASFkKAQEhRCgkJIUAgJCiFBISQohASFkKAQEhRCgkJIUAgJCiFBISQohASFkKAQEhRCgkJIUAgJCiFBISQohASFkKAQEhRCgkJIUAgJCiFBISQohASFkKAQEhRCgkJIUAgJCiFBISQohASFkKAQEhRCgkJIUAgJOOw5bM44eWbPidtbJ2fHXmETf3L23LFX2FUnQqIQEhRCgkJIUAgJCiFBISQohASFkKAQEhRCgkJIUAgJCiFBISQohASFkKAQEhRCgkJIUAgJCiHBBUKYmXtn5r/NzG/PzMdm5l+eP/76mfnIzHzi/PZ126+bbOMiJ8Iz+O611rfjO/CemfkufACPrbUewGPn95O70kuGsJ735fO7187/W3gvHj1//FG8b4sFkz1c6HuEmTmdmd/C0/jIWuvX8ea11lNwfvumW7zvwzPz+Mw8fvMrX7mktZPLdaEQ1lo311rfgbfiHTPzbRcdsNZ6ZK314FrrwdP77rvNNZNtvayfGq21Po9fwXvwmZm5H85vn77s5ZK9XOSnRm+cmdeev/wK/C18HB/GQ+dv9hA+tNGOyeYucjXs+/HozJx6PpwPrrV+YWZ+DR+cmffjU/jeDfdMNvWSIay1/ge+80Ue/yzetcVSyd76zXKiEBIUQoJCSFAICQohQSEkKIQEhZCgEBIUQoJCSFAICQohQSEkKIQEhZCgEBIUQoJCSFAICS52OZfLszh9ZnYdubWz07Njr7CJL62r+bxupRMhUQgJCiFBISQohASFkKAQEhRCgkJIUAgJCiFBISQohASFkKAQEhRCgkJIUAgJCiFBISQohASFkKAQEhRCgkJIUAgJCiFBISQohASFkKAQEhRCgkJIUAgJCiFBISQohASFkKAQEhRCgkJIUAgJCiFBISQohASFkKAQEhRCgkJIUAgJCiFBISTgsOewWZw+u+fE7Z0czo69wiY+f7brp8bRdSIkCiFBISQohASFkKAQEhRCgkJIUAgJCiFBISQohASFkKAQEhRCgkJIUAgJCiFBISQohASFkKAQEhRCgkJIUAgJCiFBISQohASFkKAQEhRCgkJIUAgJCiFBISQohASFkKAQEhRCgkJIUAgJCiFBISQohASFkKAQEhRCgkJIUAgJCiFBISQohASFkIDDrtPOOHlm14nbOzk79gab+PzZvcdeYVedCIlCSFAICQohQSEkKIQEhZCgEBIUQoJCSFAICQohQSEkKIQEhZCgEBIUQoJCSFAICQohQSEkKIQEhZCgEBIUQoJCSFAICQohQSEkKIQEhZCgEBIUQoJCSFAICQohQSEkKIQEhZCgEBIUQoJCSFAICQohQSEkKIQEhZCgEBIUQoJCSFAICQohAYc9h83i5Lk9J27v5PTmsVfYxOduvurYK+yqEyFRCAkKIUEhJCiEBIWQoBASFEKCQkhQCAkKIUEhJCiEBIWQoBASFEKCQkhQCAkKIUEhJCiEBIWQoBASFEKCQkhQCAkKIUEhJCiEBIWQoBASFEKCQkhQCAkKIUEhJCiEBIWQoBASFEKCQkhQCAkKIUEhJCiEBIWQoBASFEKCQkhQCAkKIUEhJOCw67TFyY2168itXTs9O/YKm/jsjVcde4VddSIkCiFBISQohASFkKAQEhRCgkJIUAgJCiFBISQohASFkKAQEhRCgkJIUAgJCiFBISQohASFkKAQEhRCgkJIUAgJCiFBISQohASFkKAQEhRCgkJIUAgJCiFBISQohASFkKAQEhRCgkJIUAgJCiFBISQohASFkKAQEhRCgkJIUAgJCiFBISQohAQc9h44Z3tP3NbpyRV7Quc+d/O+Y6+wq06ERCEkKIQEhZCgEBIUQoJCSFAICQohQSEkKIQEhZCgEBIUQoJCSFAICQohQSEkKIQEhZCgEBIUQoJCSFAICQohQSEkKIQEhZCgEBIUQoJCSFAICQohQSEkKIQEhZCgEBIUQoJCSFAICQohQSEkKIQEhZCgEBIUQoJCSFAICQohQSEkKIQEhZCgEBJw2H3i2n3ipk7nij2hc3/03KuPvcKuOhEShZCgEBIUQoJCSFAICQohQSEkKIQEhZCgEBIUQoJCSFAICQohQSEkKIQEhZCgEBIUQoJCSFAICQohQSEkKIQEhZCgEBIUQoJCSFAICQohQSEkKIQEhZCgEBIUQoJCSFAICQohQSEkKIQEhZCgEBIUQoJCSFAICQohQSEkKIQEhZCgEBIUQgIOu0+8Yumdnpwde4VNfO7Z+469wq6u2KdlcnsKIVEICQohQSEkKIQEhZCgEBIUQoJCSFAICQohQSEkKIQEhZCgEBIUQoJCSFAICQohQSEkKIQEhZCgEBIUQoJCSFAICQohQSEkKIQEhZCgEBIUQoJCSFAICQohQSEkKIQEhZCgEBIUQoJCSFAICQohQSEkKIQEhZCgEBIUQoJCSFAICQohAYddpw3riqV3ODk79gqb+MJz9x57hV1dsU/L5PYUQqIQEhRCgkJIUAgJCiFBISQohASFkKAQEhRCgkJIUAgJCiFBISQohASFkKAQEhRCgkJIUAgJCiFBISQohASFkKAQEhRCgkJIUAgJCiFBISQohASFkKAQEhRCgkJIUAgJCiFBISQohASFkKAQEhRCgkJIUAgJCiFBISQohASFkKAQEhRCgkJIwGHvgWeH2Xvkpk6sY6+wiS8884pjr7CrToREISQohASFkKAQEhRCgkJIUAgJCiFBISQohASFkKAQEhRCgkJIUAgJCiFBISQohASFkKAQEhRCgpcRwsyczsx/n5lfOL//+pn5yMx84vz2ddutmWzr5ZwIP4wnXnD/A3hsrfUAHju/n9yVLhTCzLwVfxf/5gUPvxePnr/8KN53qZslO7roifCv8M9w9oLH3rzWegrOb9/0Yu84Mw/PzOMz8/iNr37lTnZNNvOSIczM38PTa63fuJ0Ba61H1loPrrUePLzivtv5n0g2d5Frn74Tf39mvgf34jUz8+/xmZm5f6311Mzcj6e3XDTZ0kueCGutf77Weuta6+34PvyXtdY/xIfx0PmbPYQPbbZlsrE7+T3Cj+HdM/MJvPv8fnJXelmXhV9r/Qp+5fzlz+Jdl79Ssr9+s5wohASFkKAQEhRCgkJIUAgJCiFBISQohASFkKAQEhRCgkJIUAgJCiFBISQohASFkKAQEhRCgpd5FYs7tYazXSdub2Yde4VNfPnZe469wq46ERKFkKAQEhRCgkJIUAgJCiFBISQohASFkKAQEhRCgkJIUAgJCiFBISQohASFkKAQEhRCgkJIUAgJCiFBISQohASFkKAQEhRCgkJIUAgJCiFBISQohASFkKAQEhRCgkJIUAgJCiFBISQohASFkKAQEhRCgkJIUAgJCiFBISQohASFkKAQEhRCAg67ThvOru06Mbfpq8/t+6lxbJ0IiUJIUAgJCiFBISQohASFkKAQEhRCgkJIUAgJCiFBISQohASFkKAQEhRCgkJIUAgJCiFBISQohASFkKAQEhRCgkJIUAgJCiFBISQohASFkKAQEhRCgkJIUAgJCiFBISQohASFkKAQEhRCgkJIUAgJCiFBISQohASFkKAQEhRCgkJIUAgJCiFBISTgsOu04ezarhNzm5557uvrA9WJkCiEBIWQoBASFEKCQkhQCAkKIUEhJCiEBIWQoBASFEKCQkhQCAkKIUEhJCiEBIWQoBASFEKCQkhQCAkKIUEhJCiEBIWQoBASFEKCQkhQCAkKIUEhJCiEBIWQoBASFEKCQkhQCAkKIUEhJCiEBIWQoBASFEKCQkhQCAkKIUEhJCiEBIWQoBAScNhz2Drh7J49J+Z23bjx9fVv5NfXs01uoRAShZCgEBIUQoJCSFAICQohQSEkKIQEhZCgEBIUQoJCSFAICQohQSEkKIQEhZCgEBIUQoJCSFAICQohQSEkKIQEhZCgEBIUQoJCSFAICQohQSEkKIQEhZCgEBIUQoJCSFAICQohQSEkKIQEhZCgEBIUQoJCSFAICQohQSEkKIQEhZCgEBJw2HXacPOetevIra01x15hEzdvnB57hV11IiQKIUEhJCiEBIWQoBASFEKCQkhQCAkKIUEhJCiEBIWQoBASFEKCQkhQCAkKIUEhJCiEBIWQoBASFEKCQkhQCAkKIUEhJCiEBIWQoBASFEKCQkhQCAkKIUEhJCiEBIWQoBASFEKCQkhQCAkKIUEhJCiEBIWQoBASFEKCQkhQCAkKIUEhJCiEBBz2HLaGs+trz5G5TWdnc+wVdtWJkCiEBIWQoBASFEKCQkhQCAkKIUEhJCiEBIWQoBASFEKCQkhQCAkKIUEhJCiEBIWQoBASFEKCQkhQCAkKIUEhJCiEBIWQoBASFEKCQkhQCAkKIUEhJCiEBIWQoBASFEKCQkhQCAkKIUEhJCiEBIWQoBASFEKCQkhQCAkKIUEhJCiEBIWQoBASFEICDrtOO+HsnrXryK2dmWOvsIl1djWf1610IiQKIUEhJCiEBIWQoBASFEKCQkhQCAkKIUEhJCiEBIWQoBASFEKCQkhQCAkKIUEhJCiEBIWQoBASFEKCQkhQCAkKIUEhJCiEBIWQoBASFEKCQkhQCAkKIUEhJCiEBIWQoBASFEKCQkhQCAkKIUEhJCiEBIWQoBASFEKCQkhQCAkKIUEhJCiEBBx2nTbLun6268itna059grbOLuiz+sWOhEShZCgEBIUQoJCSFAICQohQSEkKIQEhZCgEBIUQoJCSFAICQohQSEkKIQEhZCgEBIUQoJCSFAICQohQSEkKIQEhZCgEBIUQoJCSFAICQohQSEkKIQEhZCgEBIUQoJCSFAICQohQSEkKIQEhZCgEBIUQoJCSFAICQohQSEkKIQEhZCgEBIUQgIOu04b5p6bu47c2tmaY6+wibWOvcG+OhEShZCgEBIUQoJCSFAICQohQSEkKIQEhZCgEBIUQoJCSFAICQohQSEkKIQEhZCgEBIUQoJCSFAICQohQSEkKIQEhZCgEBIUQoJCSFAICQohQSEkKIQEhZCgEBIUQoJCSFAICQohQSEkKIQEhZCgEBIUQoJCSFAICQohQSEkKIQEhZCgEBIUQoJCSMBhz2Fzshyu39xz5ObWmmOvsI2zK/q8bqETIVEICQohQSEkKIQEhZCgEBIUQoJCSFAICQohQSEkKIQEhZCgEBIUQoJCSFAICQohQSEkKIQEhZDggpdzmZnfx5dwEzfWWg/OzOvxH/F2/D7+wVrrj7dZM9nWyzkR/uZa6zvWWg+e3/8AHltrPYDHzu8nd6U7+dLovXj0/OVH8b473iY5kouGsPCfZ+Y3Zubh88fevNZ6Cs5v3/Ri7zgzD8/M4zPz+M0vfuXON042cNFLPr5zrfXpmXkTPjIzH7/ogLXWI3gE7v3rb1m3sWOyuQudCGutT5/fPo2fxzvwmZm5H85vn95qyWRrLxnCzNw3M6/+05fxt/E7+DAeOn+zh/ChrZZMtnaRL43ejJ+fmT99+/+w1vpPM/NRfHBm3o9P4Xu3WzPZ1kuGsNb6PXz7izz+Wbxri6WSvfWb5UQhJCiEBIWQoBASFEKCQkhQCAkKIUEhJCiEBIWQoBASFEKCQkhQCAkKIUEhJCiEBIWQoBASMGvtd/G5mfm/+IOdxv0V/NFOs/bSc7ozf22t9cYXe8WuIexpZh5/wZW7r4Se03b60ihRCAmudgiPHHuBDfScNnJlv0dIXo6rfCIkF3blQpiZ98zM787MJ2fmSvxdt5n5yZl5emZ+59i7XJaZedvM/PLMPDEzH5uZHz7qPlfpS6OZOcX/xrvxJD6K719r/a+jLnaHZuZv4Mv4d2utbzv2Ppfh/I/L3L/W+s3zv7/xG3jfsT5WV+1EeAc+udb6vbXWs/gZz//Rw7vaWutX8blj73GZ1lpPrbV+8/zlL+EJvOVY+1y1EN6CP3zB/Scd8f/cXMzMvB3fiV8/1g5XLYR5kceuztd+V9DMvAo/ix9Za33xWHtctRCexNtecP+t+PSRdslLmJlrno/gp9ZaP3fMXa5aCB/FAzPzTTNzHd/n+T96mL9k5vk/yvcTeGKt9ePH3udKhbDWuoEfwi95/puvD661Pnbcre7czPw0fg3fPDNPnv8Bx7vdO/ED+O6Z+a3z/77nWMtcqR+fJrfrSp0Iye0qhEQhJCiEBIWQoBASFEKCQkjA/wPQaUesu2A5IgAAAABJRU5ErkJggg==\n",
      "text/plain": [
       "<Figure size 2160x3600 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.figure(figsize=(30, 50))\n",
    "plt.imshow(gray_scale)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "90f95eed",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
